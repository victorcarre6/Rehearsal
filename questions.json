{
  "questions": [
    {
      "id": 1,
      "question": "What is Data Science?",
      "answer": "Data Science is a **blend of various tools, algorithms, and machine learning principles** with the goal to **discover hidden patterns from the raw data**. The answer lies in the difference between **explaining** and **predicting**."
    },
    {
      "id": 2,
      "question": "What is Selection Bias?",
      "answer": "Selection bias is a kind of error that occurs when the researcher decides who is going to be studied. It is usually associated with research where the selection of participants isn't random. It is sometimes referred to as the selection effect. It is the distortion of statistical analysis, resulting from the method of collecting samples.\n\n**Types of selection bias:**\n\n1. **Sampling bias**: Systematic error due to a non-random sample of a population causing some members to be less likely to be included than others\n2. **Time interval**: A trial may be terminated early at an extreme value, but the extreme value is likely to be reached by the variable with the largest variance\n3. **Data**: When specific subsets of data are chosen to support a conclusion or rejection of bad data on arbitrary grounds\n4. **Attrition**: Selection bias caused by attrition (loss of participants) discounting trial subjects/tests that did not run to completion"
    },
    {
      "id": 3,
      "question": "What is bias-variance trade-off?",
      "answer": "**Bias**: Error introduced in your model due to oversimplification of the machine learning algorithm. It can lead to underfitting.\n\n- Low bias algorithms: Decision Trees, k-NN and SVM\n- High bias algorithms: Linear Regression, Logistic Regression\n\n**Variance**: Error introduced due to complex machine learning algorithm, your model learns noise from the training data and performs badly on test data. It can lead to overfitting.\n\n**Bias-Variance trade-off**: The goal of any supervised machine learning algorithm is to have low bias and low variance to achieve good prediction performance. There is no escaping the relationship between bias and variance - increasing bias will decrease variance and vice versa."
    },
    {
      "id": 4,
      "question": "What is a confusion matrix?",
      "answer": "The confusion matrix is a 2X2 table that contains 4 outputs provided by the binary classifier:\n\n- **True-positive (TP)**: Correct positive prediction\n- **False-positive (FP)**: Incorrect positive prediction\n- **True-negative (TN)**: Correct negative prediction\n- **False-negative (FN)**: Incorrect negative prediction\n\n**Basic measures derived:**\n\n1. Error Rate = (FP+FN)/(P+N)\n2. Accuracy = (TP+TN)/(P+N)\n3. Sensitivity (Recall) = TP/P\n4. Specificity = TN/N\n5. Precision = TP/(TP+FP)\n6. F-Score = (1+b)(PREC.REC)/(b²PREC+REC)"
    },
    {
      "id": 5,
      "question": "What is the difference between 'long' and 'wide' format data?",
      "answer": "In the **wide-format**, a subject's repeated responses will be in a single row, and each response is in a separate column.\n\nIn the **long-format**, each row is a one-time point per subject.\n\nYou can recognize data in wide format by the fact that columns generally represent groups."
    },
    {
      "id": 6,
      "question": "What do you understand by the term Normal Distribution?",
      "answer": "Data distributed around a central value without any bias to the left or right reaches normal distribution in the form of a bell-shaped curve.\n\n**Properties of Normal Distribution:**\n\n1. Unimodal - one mode\n2. Symmetrical - left and right halves are mirror images\n3. Bell-shaped - maximum height (mode) at the mean\n4. Mean, Mode, and Median are all located in the center\n5. Asymptotic"
    },
    {
      "id": 7,
      "question": "What is correlation and covariance in statistics?",
      "answer": "**Correlation**: The best technique for measuring and estimating the quantitative relationship between two variables. Correlation measures how strongly two variables are related.\n\n**Covariance**: A measure that indicates the extent to which two random variables change in cycle. It explains the systematic relation between a pair of random variables, wherein changes in one variable reciprocal by a corresponding change in another variable."
    },
    {
      "id": 8,
      "question": "What is the difference between Point Estimates and Confidence Interval?",
      "answer": "**Point Estimation**: Gives us a particular value as an estimate of a population parameter. Method of Moments and Maximum Likelihood estimator methods are used.\n\n**Confidence Interval**: Gives us a range of values which is likely to contain the population parameter. Generally preferred as it tells us how likely this interval is to contain the population parameter. The probability is called Confidence Level (1 - alpha)."
    },
    {
      "id": 9,
      "question": "What is the goal of A/B Testing?",
      "answer": "A/B Testing is a hypothesis testing for a randomized experiment with two variables A and B.\n\nThe goal is to identify any changes to the web page to maximize or increase the outcome of interest. It can be used to test everything from website copy to sales emails to search ads.\n\nExample: identifying the click-through rate for a banner ad."
    },
    {
      "id": 10,
      "question": "What is p-value?",
      "answer": "p-value is a number between 0 and 1 that helps determine the strength of your results in hypothesis testing. The claim on trial is called the Null Hypothesis.\n\n- **Low p-value (≤ 0.05)**: Strong evidence against the null hypothesis - reject it\n- **High p-value (≥ 0.05)**: Strong evidence for the null hypothesis - accept it\n- **p-value = 0.05**: Hypothesis could go either way"
    },
    {
      "id": 11,
      "question": "In any 15-minute interval, there is a 20% probability that you will see at least one shooting star. What is the probability that you see at least one shooting star in the period of an hour?",
      "answer": "P(not seeing any star in 15 min) = 1 - 0.2 = 0.8\n\nP(not seeing any star in 1 hour) = (0.8)^4 = 0.4096\n\nP(seeing at least one star in 1 hour) = 1 - 0.4096 = **0.5904**"
    },
    {
      "id": 12,
      "question": "How can you generate a random number between 1–7 with only a die?",
      "answer": "- Roll the die twice to get 36 different outcomes\n- Reduce 36 to 35 (divisible by 7) by excluding combination (6,6)\n- If (6,6) appears, roll again\n- Divide remaining combinations from (1,1) to (6,5) into 7 parts of 5 each\n- All seven sets of outcomes are equally likely"
    },
    {
      "id": 13,
      "question": "A certain couple tells you that they have two children, at least one of which is a girl. What is the probability that they have two girls?",
      "answer": "For two children, there are 4 possibilities: BB, BG, GB, GG\n\nExcluding BB (since at least one is a girl), we have: BG, GB, GG\n\nP(two girls | at least one girl) = **1/3**"
    },
    {
      "id": 14,
      "question": "A jar has 1000 coins, of which 999 are fair and 1 is double headed. Pick a coin at random, and toss it 10 times. Given that you see 10 heads, what is the probability that the next toss of that coin is also a head?",
      "answer": "P(fair coin) = 999/1000 = 0.999\nP(unfair coin) = 1/1000 = 0.001\n\nP(10 heads | fair) = 0.999 × (1/2)^10 = 0.000976\nP(10 heads | unfair) = 0.001 × 1 = 0.001\n\nP(fair | 10 heads) = 0.000976 / 0.001976 = 0.4939\nP(unfair | 10 heads) = 0.001 / 0.001976 = 0.5061\n\nP(next head) = 0.4939 × 0.5 + 0.5061 × 1 = **0.7531**"
    },
    {
      "id": 15,
      "question": "What do you understand by statistical power of sensitivity and how do you calculate it?",
      "answer": "Sensitivity is commonly used to validate the accuracy of a classifier (Logistic, SVM, Random Forest etc.).\n\nSensitivity = \"Predicted True events / Total events\"\n\n**Formula**: Sensitivity = True Positives / Positives in Actual Dependent Variable"
    },
    {
      "id": 16,
      "question": "Why Is Re-sampling Done?",
      "answer": "Resampling is done for:\n\n- Estimating the accuracy of sample statistics by using subsets of accessible data or drawing randomly with replacement\n- Substituting labels on data points when performing significance tests\n- Validating models by using random subsets (bootstrapping, cross-validation)"
    },
    {
      "id": 17,
      "question": "What are the differences between over-fitting and under-fitting?",
      "answer": "**Overfitting**: A statistical model describes random error or noise instead of the underlying relationship. Occurs when a model is excessively complex (too many parameters relative to observations). Results in poor predictive performance.\n\n**Underfitting**: A model cannot capture the underlying trend of the data. Example: fitting a linear model to non-linear data. Also results in poor predictive performance."
    },
    {
      "id": 18,
      "question": "How to combat Overfitting and Underfitting?",
      "answer": "- Resample the data to estimate model accuracy (k-fold cross-validation)\n- Have a validation dataset to evaluate the model"
    },
    {
      "id": 19,
      "question": "What is regularisation? Why is it useful?",
      "answer": "Regularisation is the process of adding a tuning parameter to a model to induce smoothness in order to prevent overfitting.\n\nThis is done by adding a constant multiple to an existing weight vector, often L1 (Lasso) or L2 (Ridge).\n\nThe model predictions should then minimize the loss function calculated on the regularized training set."
    },
    {
      "id": 20,
      "question": "What Is the Law of Large Numbers?",
      "answer": "A theorem that describes the result of performing the same experiment a large number of times.\n\nIt forms the basis of frequency-style thinking and states that the sample means, sample variance, and sample standard deviation converge to what they are trying to estimate."
    },
    {
      "id": 21,
      "question": "What Are Confounding Variables?",
      "answer": "A confounder is a variable that influences both the dependent variable and independent variable.\n\n**Example**: If researching whether lack of exercise leads to weight gain:\n\n- Independent variable: lack of exercise\n- Dependent variable: weight gain\n- Confounding variable: age of the subject"
    },
    {
      "id": 22,
      "question": "What Are the Types of Biases That Can Occur During Sampling?",
      "answer": "- Selection bias\n- Under coverage bias\n- Survivorship bias"
    },
    {
      "id": 23,
      "question": "What is Survivorship Bias?",
      "answer": "The logical error of focusing on aspects that support surviving some process and casually overlooking those that did not work because of their lack of prominence.\n\nThis can lead to wrong conclusions in numerous different ways."
    },
    {
      "id": 24,
      "question": "What is selection Bias?",
      "answer": "Selection bias occurs when the sample obtained is not representative of the population intended to be analysed."
    },
    {
      "id": 25,
      "question": "Explain how a ROC curve works?",
      "answer": "The ROC curve is a graphical representation of the contrast between true positive rates and false-positive rates at various thresholds.\n\nIt is often used as a proxy for the trade-off between sensitivity (true positive rate) and false-positive rate."
    },
    {
      "id": 26,
      "question": "What is TF/IDF vectorization?",
      "answer": "TF-IDF (Term Frequency-Inverse Document Frequency) is a numerical statistic that reflects how important a word is to a document in a collection or corpus.\n\nIt is often used as a weighting factor in information retrieval and text mining.\n\nThe TF-IDF value increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word in the corpus."
    },
    {
      "id": 27,
      "question": "Why we generally use Softmax non-linearity function as last operation in-network?",
      "answer": "Because it takes in a vector of real numbers and returns a probability distribution.\n\nEach element of the output is non-negative and the sum over all components is 1.\n\nSoftmax(x)_i = e^(x_i) / Σ(e^(x_j))"
    },
    {
      "id": 28,
      "question": "Python or R – Which one would you prefer for text analytics?",
      "answer": "Python is preferred because:\n\n- Python has Pandas library with easy-to-use data structures and high-performance data analysis tools\n- R is more suitable for machine learning than just text analysis\n- Python performs faster for all types of text analytics"
    },
    {
      "id": 29,
      "question": "How does data cleaning play a vital role in the analysis?",
      "answer": "- Cleaning data from multiple sources helps transform it into a format that data analysts can work with\n- Data Cleaning helps increase the accuracy of machine learning models\n- It's a cumbersome process - time increases exponentially with more data sources\n- It might take up to 80% of the time for just cleaning data"
    },
    {
      "id": 30,
      "question": "Differentiate between univariate, bivariate and multivariate analysis.",
      "answer": "**Univariate**: Involves only one variable. Purpose is to describe the data and find patterns using mean, median, mode, dispersion, etc.\n\n**Bivariate**: Involves two different variables. Deals with causes and relationships between the two variables.\n\n**Multivariate**: Involves three or more variables. Similar to bivariate but contains more than one dependent variable."
    },
    {
      "id": 31,
      "question": "Explain Star Schema.",
      "answer": "A traditional database schema with a central table. Satellite tables map IDs to physical names or descriptions and can be connected to the central fact table using ID fields.\n\nThese tables are known as lookup tables and are principally useful in real-time applications as they save memory. Sometimes involves several layers of summarization to recover information faster."
    },
    {
      "id": 32,
      "question": "What is Cluster Sampling?",
      "answer": "A technique used when it becomes difficult to study the target population spread across a wide area and simple random sampling cannot be applied.\n\nCluster Sample is a probability sample where each sampling unit is a collection or cluster of elements.\n\n**Example**: Dividing Japan into cities (clusters) to survey academic performance of high school students."
    },
    {
      "id": 33,
      "question": "What is Systematic Sampling?",
      "answer": "A statistical technique where elements are selected from an ordered sampling frame.\n\nThe list is progressed in a circular manner - once you reach the end, it continues from the top again.\n\nThe best example is the equal probability method."
    },
    {
      "id": 34,
      "question": "What are Eigenvectors and Eigenvalues?",
      "answer": "**Eigenvectors**: Used for understanding linear transformations. Usually calculated for correlation or covariance matrices. They are the directions along which a linear transformation acts by flipping, compressing, or stretching.\n\n**Eigenvalues**: The strength of the transformation in the direction of eigenvector, or the factor by which the compression occurs."
    },
    {
      "id": 35,
      "question": "Can you cite some examples where a false positive is important than a false negative?",
      "answer": "**Example 1 (Medical)**: A patient tested positive for cancer but doesn't actually have it. Starting chemotherapy would damage healthy cells and might lead to severe diseases.\n\n**Example 2 (E-commerce)**: Sending $1000 gift vouchers to customers wrongly marked as $10,000+ purchasers causes financial loss."
    },
    {
      "id": 36,
      "question": "Can you cite some examples where a false negative important than a false positive?",
      "answer": "**Example 1 (Airport Security)**: A true threat flagged as non-threat by the predictive model\n\n**Example 2 (Legal)**: A judge decides to make a criminal go free\n\n**Example 3 (Personal)**: Rejecting to marry a good person based on a predictive model"
    },
    {
      "id": 37,
      "question": "Can you cite some examples where both false positive and false negatives are equally important?",
      "answer": "**Banking industry**: Giving loans is the primary source of making money, but repayment rate is crucial.\n\nBanks don't want to lose good customers (false negative) and don't want to acquire bad customers (false positive). Both become very important to measure."
    },
    {
      "id": 38,
      "question": "Can you explain the difference between a Validation Set and a Test Set?",
      "answer": "**Validation Set**: Part of the training set, used for parameter selection and to avoid overfitting.\n\n**Test Set**: Used for testing or evaluating the performance of a trained model.\n\nTraining set fits parameters (weights), test set assesses performance (predictive power and generalization)."
    },
    {
      "id": 39,
      "question": "Explain cross-validation.",
      "answer": "A model validation technique for evaluating how the outcomes of statistical analysis will generalize to an independent dataset.\n\nThe goal is to test the model in the training phase using a validation data set to limit problems like overfitting and get insight on how the model will generalize."
    },
    {
      "id": 40,
      "question": "What is Machine Learning?",
      "answer": "Machine Learning explores the study and construction of algorithms that can learn from and make predictions on data.\n\nClosely related to computational statistics. Used to devise complex models and algorithms for predictive analytics."
    },
    {
      "id": 41,
      "question": "What is Supervised Learning?",
      "answer": "The machine learning task of inferring a function from labeled training data.\n\n**Algorithms**: Support Vector Machines, Regression, Naive Bayes, Decision Trees, K-nearest Neighbor, Neural Networks\n\n**Example**: Fruit classifier with labels \"this is an orange, this is an apple, this is a banana\""
    },
    {
      "id": 42,
      "question": "What is Unsupervised learning?",
      "answer": "A type of machine learning algorithm used to draw inferences from datasets consisting of input data without labelled responses.\n\n**Algorithms**: Clustering, Anomaly Detection, Neural Networks, Latent Variable Models\n\n**Example**: Fruit clustering as \"fruits with soft skin\", \"fruits with shiny hard skin\", \"elongated yellow fruits\""
    },
    {
      "id": 43,
      "question": "What are the various classification algorithms?",
      "answer": "- Linear\n- Decision Trees\n- SVM\n- Kernel Estimation\n- Neural Networks\n- Quadratic\n- Naive Bayes\n- Logistic Regression\n- Recurrent Neural Network (RNN)\n- Modular Neural Network"
    },
    {
      "id": 44,
      "question": "What is 'Naive' in a Naive Bayes?",
      "answer": "The Naive Bayes Algorithm is based on Bayes' theorem, which describes the probability of an event based on prior knowledge of conditions.\n\nThe Algorithm is 'naive' because it makes assumptions that may or may not turn out to be correct."
    },
    {
      "id": 45,
      "question": "Explain SVM algorithm in detail.",
      "answer": "SVM (Support Vector Machine) is a supervised machine learning algorithm used for both Regression and Classification.\n\nIf you have n features, SVM tries to plot it in n-dimensional space with each feature being a coordinate value.\n\nSVM uses hyperplanes to separate out different classes based on the provided kernel function."
    },
    {
      "id": 46,
      "question": "What are the support vectors in SVM?",
      "answer": "Support vectors are the data points closest to the classifier (hyperplane).\n\nThe distance from the classifier to these closest data points is marked by thinner lines.\n\nThe distance between the two thin lines is called the margin."
    },
    {
      "id": 47,
      "question": "What are the different kernels in SVM?",
      "answer": "1. Linear Kernel\n2. Polynomial kernel\n3. Radial basis kernel\n4. Sigmoid kernel"
    },
    {
      "id": 48,
      "question": "Explain Decision Tree algorithm in detail.",
      "answer": "A supervised machine learning algorithm mainly used for Regression and Classification.\n\nIt breaks down a data set into smaller subsets while an associated decision tree is incrementally developed.\n\nThe final result is a tree with decision nodes and leaf nodes. Can handle both categorical and numerical data."
    },
    {
      "id": 49,
      "question": "What are Entropy and Information gain in Decision tree algorithm?",
      "answer": "The core algorithm for building a decision tree is called ID3, which uses Entropy and Information Gain.\n\n**Entropy**: Checks homogeneity of a sample. If completely homogeneous, entropy = 0. If equally divided, entropy = 1.\n\n**Information Gain**: Based on the decrease in entropy after a dataset is split on an attribute. Constructing a decision tree is about finding attributes with the highest information gain."
    },
    {
      "id": 50,
      "question": "What is pruning in Decision Tree?",
      "answer": "Pruning is a technique that reduces the size of decision trees by removing sections that provide little power to classify instances.\n\nWhen we remove sub-nodes of a decision node, this process is called pruning (opposite process of splitting)."
    },
    {
      "id": 69,
      "question": "What do you mean by Deep Learning?",
      "answer": "Deep Learning is a paradigm of machine learning which has shown incredible promise in recent years.\n\nThis is because Deep Learning shows a great analogy with the functioning of the human brain."
    },
    {
      "id": 70,
      "question": "What is the difference between machine learning and deep learning?",
      "answer": "**Machine learning**: A field of computer science that gives computers the ability to learn without being explicitly programmed. Categories: Supervised, Unsupervised, Reinforcement learning.\n\n**Deep Learning**: A subfield of machine learning concerned with algorithms inspired by the structure and function of the brain called artificial neural networks."
    },
    {
      "id": 71,
      "question": "What, in your opinion, is the reason for the popularity of Deep Learning in recent times?",
      "answer": "Two main reasons:\n\n1. The increase in the amount of data generated through various sources\n2. The growth in hardware resources (GPUs) required to run these models\n\nGPUs are multiple times faster and help build bigger and deeper models in less time."
    },
    {
      "id": 72,
      "question": "What is reinforcement learning?",
      "answer": "Learning what to do and how to map situations to actions to maximise the numerical reward signal.\n\nThe learner is not told which action to take but must discover which action yields maximum reward.\n\nInspired by human learning, based on the reward/penalty mechanism."
    },
    {
      "id": 73,
      "question": "What are Artificial Neural Networks?",
      "answer": "A specific set of algorithms that have revolutionized machine learning, inspired by biological neural networks.\n\nNeural Networks can adapt to changing input so the network generates the best possible result without needing to redesign the output criteria."
    },
    {
      "id": 74,
      "question": "Describe the structure of Artificial Neural Networks?",
      "answer": "Works on the same principle as biological Neural Networks.\n\nConsists of inputs which get processed with weighted sums and Bias, with the help of Activation Functions."
    },
    {
      "id": 75,
      "question": "How Are Weights Initialized in a Network?",
      "answer": "**Initialize to 0**: Makes model similar to a linear model. All neurons perform same operation - makes deep net useless.\n\n**Initialize randomly**: Weights assigned randomly very close to 0. Gives better accuracy since every neuron performs different computations. Most commonly used method."
    },
    {
      "id": 76,
      "question": "What Is the Cost Function?",
      "answer": "Also referred to as \"loss\" or \"error,\" it's a measure to evaluate how good your model's performance is.\n\nUsed to compute the error of the output layer during backpropagation, which is then used during training functions."
    },
    {
      "id": 77,
      "question": "What Are Hyperparameters?",
      "answer": "Parameters whose values are set before the learning process begins.\n\nDetermines how a network is trained and the structure of the network (number of hidden units, learning rate, epochs, etc.)."
    },
    {
      "id": 78,
      "question": "What Will Happen If the Learning Rate Is Set inaccurately (Too Low or Too High)?",
      "answer": "**Too Low**: Training progresses very slowly due to minimal weight updates. Takes many updates before reaching minimum.\n\n**Too High**: Causes divergent behaviour due to drastic weight updates. May fail to converge or even diverge."
    },
    {
      "id": 79,
      "question": "What Is the Difference Between Epoch, Batch, and Iteration in Deep Learning?",
      "answer": "**Epoch**: One iteration over the entire dataset\n\n**Batch**: When we cannot pass entire dataset at once, we divide it into several batches\n\n**Iteration**: Number of batches needed to complete one epoch. (10,000 images / 200 batch size = 50 iterations)"
    },
    {
      "id": 80,
      "question": "What Are the Different Layers on CNN?",
      "answer": "1. **Convolutional Layer**: Performs convolution operation, creating smaller picture windows\n2. **ReLU Layer**: Brings non-linearity, converts negative pixels to zero\n3. **Pooling Layer**: Down-sampling operation that reduces dimensionality\n4. **Fully Connected Layer**: Recognizes and classifies objects in the image"
    },
    {
      "id": 81,
      "question": "What Is Pooling on CNN, and How Does It Work?",
      "answer": "Pooling is used to reduce the spatial dimensions of a CNN.\n\nIt performs down-sampling operations to reduce dimensionality and creates a pooled feature map by sliding a filter matrix over the input matrix."
    },
    {
      "id": 82,
      "question": "What are Recurrent Neural Networks(RNNs)?",
      "answer": "A type of artificial neural networks designed to recognise patterns from sequences of data (time series, stock market, etc.).\n\nRNNs take as input not just the current example but also what they perceived previously in time.\n\nThe decision at time t-1 affects the decision at time t. They have two sources of input: present and recent past."
    },
    {
      "id": 83,
      "question": "How Does an LSTM Network Work?",
      "answer": "Long-Short-Term Memory (LSTM) is a special RNN capable of learning long-term dependencies.\n\n**Three steps**:\n\n1. The network decides what to forget and what to remember\n2. It selectively updates cell state values\n3. The network decides what part of the current state makes it to the output"
    },
    {
      "id": 84,
      "question": "What Is a Multi-layer Perceptron(MLP)?",
      "answer": "Has an input layer, hidden layer(s), and output layer. Same structure as single layer perceptron but with one or more hidden layers.\n\nSingle layer perceptron: only linear separable classes with binary output\nMLP: can classify nonlinear classes\n\nUses backpropagation for supervised learning."
    },
    {
      "id": 85,
      "question": "Explain Gradient Descent.",
      "answer": "**Gradient**: Measures how much the output of a function changes if you change the inputs a little bit. Measures the change in all weights with regard to the change in error.\n\n**Gradient Descent**: A minimization algorithm that minimizes a given function (Activation Function). Like climbing down to the bottom of a valley."
    },
    {
      "id": 86,
      "question": "What is exploding gradients?",
      "answer": "When training an RNN, exponentially growing (very large) error gradients accumulate and result in very large updates to neural network weights.\n\nValues can become so large as to overflow and result in NaN values.\n\nEffect: Model is unstable and unable to learn from training data."
    },
    {
      "id": 87,
      "question": "What is vanishing gradients?",
      "answer": "When training an RNN, your slope can become too small, making training difficult.\n\nLeads to long training times, poor performance, and low accuracy."
    },
    {
      "id": 89,
      "question": "What is Back Propagation and Explain it's Working.",
      "answer": "A training algorithm used for multilayer neural networks that moves error from the end of the network to all weights inside, allowing efficient computation of the gradient.\n\n**Steps**:\n\n1. Forward Propagation of Training Data\n2. Derivatives computed using output and target\n3. Back Propagate for computing derivative of error wrt output activation\n4. Using previously calculated derivatives for output\n5. Update the Weights"
    },
    {
      "id": 90,
      "question": "What are the variants of Back Propagation?",
      "answer": "**Stochastic Gradient Descent**: Uses only a single training example for gradient calculation and parameter update.\n\n**Batch Gradient Descent**: Calculates gradient for whole dataset and performs update at each iteration.\n\n**Mini-batch Gradient Descent**: Most popular. Uses mini-batch of samples instead of single example."
    },
    {
      "id": 91,
      "question": "What is a matrix in linear algebra?",
      "answer": "A matrix is a rectangular array of numbers, symbols or expressions arranged in rows & columns.\n\nIt is represented by a 2D ndarray in NumPy.\n\n**Key properties:**\n- Dimensions are given as (rows × columns)\n- Can be added/multiplied following specific rules\n- The transpose switches rows and columns: shape (3,4) becomes (4,3)"
    },
    {
      "id": 92,
      "question": "What is the inverse of a matrix? Does it always exist?",
      "answer": "A matrix A is **invertible** if there exists a matrix B such that AB = BA = I where I is the identity matrix.\n\nThe inverse is denoted A⁻¹ but it **doesn't always exist**:\n\n1. The matrix must be **square** (same number of rows and columns)\n2. The matrix must have **full rank** (rank equals the number of features)\n3. No two columns can be linearly dependent (collinearity issue)"
    },
    {
      "id": 93,
      "question": "What are the three possible scenarios when solving Ax = b in a system of linear equations?",
      "answer": "1. **No solution**: For instance, if A is the null matrix and b is a column vector full of ones\n\n2. **One unique solution**: If A is invertible, then x = A⁻¹b\n\n3. **Infinite solutions**: For example, any vector x solves Ax = b if both A and b are null"
    },
    {
      "id": 94,
      "question": "What does the derivative of a function represent?",
      "answer": "**The derivative represents the rate of change of a function with respect to its argument.**\n\nExample: Phone cost f(m) = 500 + 20*m where m is months.\n- f'(m) = 20 means each additional month costs 20 EUR more\n\nIn ML, derivatives are crucial for:\n- Computing gradients in optimization\n- Backpropagation in neural networks\n- Understanding how loss changes with parameters"
    },
    {
      "id": 95,
      "question": "How would you define Euclidean distance? Do you know other types?",
      "answer": "**Euclidean distance (L2)**: The \"ordinary\" straight-line distance between two points, measured with a ruler.\n\nFormula: d = √(Σ(xi - yi)²)\n\n**Other distances:**\n- **Manhattan (L1)**: Sum of absolute differences |x1-y1| + |x2-y2|. Used when movement is constrained to a grid (like NYC streets)\n- **Cosine similarity**: Measures angle between vectors, useful for text similarity\n- **Minkowski**: Generalization of L1 and L2"
    },
    {
      "id": 96,
      "question": "What is the expected value of a random variable?",
      "answer": "Let X be a random variable with a finite number of outcomes x₁, x₂, ..., xₙ occurring with probabilities P(x₁), P(x₂), ..., P(xₙ).\n\n**The expectation is defined as:**\nE[X] = Σ xᵢ P(xᵢ)\n\nIt represents the \"average\" value you'd expect if you repeated the experiment many times (Law of Large Numbers)."
    },
    {
      "id": 97,
      "question": "What is a Type I error vs Type II error?",
      "answer": "**Type I Error (False Positive):**\n- H₀ is actually true\n- But H₀ is rejected\n- Probability = α (significance level)\n\n**Type II Error (False Negative):**\n- H₀ is actually false\n- But H₀ is not rejected\n- Probability = β\n\n**Power of a test** = 1 - β = probability of correctly rejecting a false H₀"
    },
    {
      "id": 98,
      "question": "What is the power of a statistical test?",
      "answer": "Power is the probability of **correctly rejecting the null hypothesis** when it should be rejected.\n\n**Examples:**\n- Probability of not missing out on a great feature in A/B testing\n- Probability of detecting an effective drug in a clinical trial\n- Power = 1 - P(Type II error)\n\nHigher power means fewer false negatives."
    },
    {
      "id": 99,
      "question": "How would you design an A/B Test for a new app feature?",
      "answer": "**Experiment design steps:**\n\n1. Develop the corresponding feature\n2. Create two groups (control vs. treatment)\n3. Randomly assign users to the treatment group\n4. Deploy the feature only to the treatment group\n5. Collect behavior statistics (e.g., sample mean)\n6. Run hypothesis test to determine if the difference is statistically significant"
    },
    {
      "id": 100,
      "question": "Using NumPy, how can you find the sum of values along axis 1 of an ndarray?",
      "answer": "```python\nsum_axis1 = nd_array.sum(axis=1)\n```\n\nThis is equivalent to:\n`nd_array[:,0] + nd_array[:,1] + nd_array[:,2] + ...`\n\n**Axis explanation:**\n- axis=0: operates along rows (column-wise)\n- axis=1: operates along columns (row-wise)"
    },
    {
      "id": 101,
      "question": "How do you access specific rows and columns from a Pandas DataFrame using loc?",
      "answer": "```python\ndf.loc[5:10, ['first_name', 'last_name']]\n```\n\nThis selects rows 5 to 10 (inclusive) and columns 'first_name' and 'last_name'.\n\n**Key distinction:**\n- `loc`: label-based indexing (uses actual index values)\n- `iloc`: integer position-based indexing (uses positions 0, 1, 2...)"
    },
    {
      "id": 102,
      "question": "How do you select the third column of an ndarray starting from the second row?",
      "answer": "```python\nnd_array[1:, 2]\n```\n\n**Explanation:**\n- `1:` = from row index 1 (second row) to the end\n- `2` = column index 2 (third column)\n\nRemember: Python indexing starts at 0!"
    },
    {
      "id": 103,
      "question": "How do you perform Matrix Multiplication with NumPy?",
      "answer": "**Two equivalent ways:**\n\n```python\nnp.matmul(A, B)\n# or\nA @ B\n```\n\n**Important:** Matrix multiplication requires the inner dimensions to match:\n- A (m×n) @ B (n×p) = C (m×p)\n- If A is (3×2) and B is (3×2), AB is undefined but AᵀB works (2×3 @ 3×2 = 2×2)"
    },
    {
      "id": 104,
      "question": "What is a 'bin' or 'bucket' in data analysis?",
      "answer": "**Binning** divides the entire range of values into a series of intervals.\n\n**Properties:**\n- Bins are usually consecutive, non-overlapping intervals\n- Must be adjacent\n- Often (but not required to be) of equal size\n\n**Use cases:**\n- Histograms\n- Converting continuous variables to categorical\n- Reducing noise in data"
    },
    {
      "id": 105,
      "question": "How do you Create (CRUD) in SQL?",
      "answer": "Using the `INSERT INTO` keyword:\n\n```sql\nINSERT INTO doctors (name, age, specialty)\nVALUES ('Dr House', 42, 'Diagnostic Medicine');\n```\n\n**Note:** We don't insert the id because the database manages it automatically (auto-increment)."
    },
    {
      "id": 106,
      "question": "How do you Read (CRUD) in SQL?",
      "answer": "Using the `SELECT` keyword:\n\n```sql\n-- Fetch all doctors\nSELECT * FROM doctors;\n\n-- Fetch specific doctor\nSELECT * FROM doctors WHERE id = 3;\n\n-- Select specific columns\nSELECT name, specialty FROM doctors;\n```"
    },
    {
      "id": 107,
      "question": "How do you Update (CRUD) in SQL?",
      "answer": "Using the `UPDATE` keyword:\n\n```sql\nUPDATE doctors\nSET age = 40, name = 'John Smith'\nWHERE id = 3;\n```\n\n**Warning:** If you omit the `WHERE` clause, you will update **every record** in the table!"
    },
    {
      "id": 108,
      "question": "How do you Delete (CRUD) in SQL?",
      "answer": "Using the `DELETE` keyword:\n\n```sql\nDELETE FROM doctors WHERE id = 32;\n```\n\n**Warning:** If you omit the `WHERE` clause, you will delete **every record** in the table!"
    },
    {
      "id": 109,
      "question": "What are the 4 main types of JOIN operations in SQL?",
      "answer": "1. **INNER JOIN**: Returns only matching rows from both tables\n\n2. **LEFT JOIN**: Returns all rows from left table + matching rows from right\n\n3. **RIGHT JOIN**: Returns all rows from right table + matching rows from left\n\n4. **FULL OUTER JOIN**: Returns all rows from both tables, with NULLs where no match"
    },
    {
      "id": 110,
      "question": "What is the difference between GROUP BY and PARTITION BY in SQL?",
      "answer": "**GROUP BY**: Used in regular aggregate functions\n- Collapses rows into groups\n- Returns one row per group\n\n**PARTITION BY**: Used in window functions\n- Does NOT collapse rows\n- Performs calculations across a set of rows related to current row\n- Preserves all original rows\n\n```sql\n-- GROUP BY: one row per department\nSELECT dept, AVG(salary) FROM employees GROUP BY dept;\n\n-- PARTITION BY: all rows preserved with running average\nSELECT name, dept, AVG(salary) OVER (PARTITION BY dept) FROM employees;\n```"
    },
    {
      "id": 111,
      "question": "What is a window function in SQL?",
      "answer": "A window function performs calculations across a set of rows that are related to the current row (the \"window\").\n\n**Key components:**\n- `OVER()`: Defines the window\n- `PARTITION BY`: Divides rows into groups\n- `ORDER BY`: Orders rows within each partition\n\n**Common functions:** ROW_NUMBER(), RANK(), LAG(), LEAD(), SUM() OVER, AVG() OVER"
    },
    {
      "id": 112,
      "question": "What measure does the hierarchical structure of Decision Trees depend on?",
      "answer": "The hierarchical structure is based on the **Gini index**.\n\n**Gini index:**\n- Measures the ability of each feature to separate the data\n- Calculates the \"impurity\" of a node\n- Gini = 0: perfectly pure (all samples belong to one class)\n- Gini = 0.5: maximum impurity for binary classification\n\nAlternative: Entropy (used in ID3 algorithm)"
    },
    {
      "id": 113,
      "question": "Do Decision Trees and Random Forests require feature scaling?",
      "answer": "**No!** One advantage of Decision Trees and Random Forests is that they are **not affected by varying scales** of features.\n\n**Why?**\n- They make decisions based on thresholds, not distances\n- Splits are based on feature values, not magnitudes\n\nContrast with: SVM, KNN, Neural Networks (which DO require scaling)"
    },
    {
      "id": 114,
      "question": "What is the difference between Bagging and Boosting?",
      "answer": "**Bagging (Bootstrap Aggregating):**\n- Trains models in parallel on random subsets\n- All weak learners have equal weight in final vote\n- Reduces variance (fights overfitting)\n- Example: Random Forest\n\n**Boosting:**\n- Trains models sequentially\n- Each model focuses on errors of previous ones\n- Best learners get more weight in final vote\n- Reduces bias (fights underfitting)\n- Examples: AdaBoost, XGBoost, LightGBM"
    },
    {
      "id": 115,
      "question": "If your algorithm has high bias, which ensemble method should you use?",
      "answer": "**Boosting!**\n\nThe main objective of Boosting is to **reduce bias**.\n\n**How it works:**\n- Each new model focuses on the mistakes of previous models\n- Sequentially builds stronger and stronger models\n- Combines them with weighted voting\n\n**Caution:** Boosting is sensitive to outliers (it keeps trying to learn hard-to-predict points)"
    },
    {
      "id": 116,
      "question": "How can outliers negatively affect Boosting?",
      "answer": "Boosting spends more time trying to learn observations that are hard to predict.\n\n**The problem:**\n- If an observation is an outlier, the model may never learn to predict it correctly\n- The algorithm can get stuck trying to solve the impossible\n- This leads to overfitting on noise\n\n**Solution:** Clean outliers before boosting, or use robust boosting variants"
    },
    {
      "id": 117,
      "question": "How do you combine predictions in ensemble methods for regression vs classification?",
      "answer": "**Regression:**\n- Average the predictions from all models\n- Sometimes weighted average based on model performance\n\n**Classification:**\n- Majority voting (most common prediction wins)\n- Soft voting: average the predicted probabilities, then pick highest\n- Weighted voting: better models get more influence"
    },
    {
      "id": 118,
      "question": "What are the iterative steps of the K-means algorithm?",
      "answer": "1. **Choose K**: Decide the number of clusters to find\n2. **Initialize**: Place K centroids at random positions\n3. **Assign**: Compute distance from each point to each centroid; assign each point to nearest centroid\n4. **Update**: Compute the mean of each cluster → new centroid\n5. **Repeat** steps 3-4 until centroids stop moving (convergence)"
    },
    {
      "id": 119,
      "question": "How do you find the optimal number of clusters for K-means?",
      "answer": "**Elbow Method:**\n\n1. Run K-means for different values of K (e.g., 1 to 10)\n2. Calculate the sum of squared distances (inertia) for each K\n3. Plot K vs. inertia\n4. Look for the \"elbow\" - where adding more clusters doesn't significantly reduce inertia\n\n**Other methods:** Silhouette score, Gap statistic"
    },
    {
      "id": 120,
      "question": "How does PCA work in mathematical terms?",
      "answer": "PCA (Principal Component Analysis) works by finding the **eigenvectors** and **eigenvalues** of the covariance matrix.\n\n**Process:**\n1. Standardize the data\n2. Compute covariance matrix\n3. Find eigenvectors (principal components) and eigenvalues\n4. Eigenvectors give the **directions** of maximum variance\n5. Eigenvalues indicate the **amount of variance** explained by each component\n\n**Result:** New orthogonal axes ranked by variance explained"
    },
    {
      "id": 121,
      "question": "What are eigenvectors in the context of PCA?",
      "answer": "An eigenvector is a vector whose **direction remains unchanged** when a linear transformation is applied to it.\n\n**In PCA:**\n- They are the **orthogonal directions** along which data has maximum variance\n- They form the **new coordinate system** for dimensionality reduction\n- The first principal component captures the most variance, the second captures the most remaining variance (orthogonal to the first), etc."
    },
    {
      "id": 122,
      "question": "Where would the separating hyperplane of a max-margin SVM be located?",
      "answer": "The separating hyperplane of a max-margin classifier is located **exactly halfway** between the support vectors of each class.\n\n**Goal:** Maximize the margin (distance between hyperplane and nearest points)\n\n**Why?** A larger margin typically leads to better generalization on unseen data."
    },
    {
      "id": 123,
      "question": "When tuning KNN, what is the risk of setting K too low?",
      "answer": "Low K values lead to **overfitting** the training set.\n\n**Why?**\n- With K=1, the model memorizes training data (every point is its own neighbor)\n- Decision boundaries become too irregular\n- High variance, sensitive to noise\n\n**Rule of thumb:** K typically lies between 5 and 10, but should be tuned via cross-validation."
    },
    {
      "id": 124,
      "question": "What are the steps of a Gradient Descent epoch?",
      "answer": "1. **Compute gradient**: Calculate the partial derivative of the loss function with respect to each parameter\n\n2. **Update parameters**: Adjust each parameter by subtracting (learning_rate × gradient)\n   `θ_new = θ_old - α × ∇L(θ)`\n\n3. **Repeat**: Continue until convergence (loss stops decreasing significantly)"
    },
    {
      "id": 125,
      "question": "What is a Moving-Average (MA) process in time series?",
      "answer": "A **MA process** is one whose values are a linear combination of past **error terms** (shocks).\n\n**Characteristics:**\n- Any \"shock\" has a limited time effect\n- Shows rebound/elastic behavior\n\n**Example:** A country's GDP growth - when a pandemic shock lowers it to -10%, it may bounce back to +5% the following year."
    },
    {
      "id": 126,
      "question": "What are the 3 properties of a stationary time series?",
      "answer": "A time series is **stationary** if these properties remain constant over time:\n\n1. **Constant mean**\n2. **Constant variance**\n3. **Constant autocorrelation** (correlation between values at different time lags)\n\n**Why it matters:** Many time series models (ARIMA) assume stationarity. Non-stationary data must be transformed first."
    },
    {
      "id": 127,
      "question": "Give examples of baseline scores for classification and regression.",
      "answer": "**Classification baseline:**\n- Random prediction of target classes\n- Always predict the majority class\n- Accuracy = proportion of majority class\n\n**Regression baseline:**\n- Predict the mean of the target variable\n- Predict the median (more robust to outliers)\n- R² = 0 by definition for mean prediction\n\n**Purpose:** Any useful model should beat the baseline!"
    },
    {
      "id": 128,
      "question": "What is Ordinary Least Squares (OLS)?",
      "answer": "OLS is a type of **linear regression model** that minimizes the sum of the squares of the differences between observed values and predicted values.\n\n**Minimizes:** Σ(yᵢ - ŷᵢ)²\n\n**Distance:** The \"natural\" Euclidean distance between observed datapoints and the regression line (or hyperplane in higher dimensions)."
    },
    {
      "id": 129,
      "question": "How can you check statistical significance in linear regression?",
      "answer": "Compute **p-values** and the **F-statistic**:\n\n**p-values (for each coefficient):**\n- Test if each feature has a significant effect\n- p < 0.05 → coefficient is significant\n\n**F-statistic (for overall model):**\n- Tests if the model is better than a baseline (intercept only)\n- F close to 1 → model may not be statistically significant"
    },
    {
      "id": 130,
      "question": "How do you check goodness-of-fit in linear regression?",
      "answer": "Compute the **R² score** (coefficient of determination).\n\n**Interpretation:**\n- R² = 1: Model explains all variance\n- R² = 0: Model explains no variance (same as predicting mean)\n- R² = 0.7: Model explains 70% of variance\n\n**Caution:** R² always increases with more features → use Adjusted R² for multiple regression"
    },
    {
      "id": 131,
      "question": "How do you check inference conditions in linear regression?",
      "answer": "**Plot the residuals** and verify they are:\n\n1. **Randomly distributed** (no pattern)\n2. **Centered on 0** (mean of residuals ≈ 0)\n3. **Constant variance** (homoscedasticity)\n4. **No autocorrelation** (residuals independent of each other)\n\n**Visual checks:** Residuals vs. fitted values plot, Q-Q plot for normality"
    },
    {
      "id": 132,
      "question": "What is the size of the residuals vector in linear regression?",
      "answer": "The residual vector **u** is of size **n** (number of observations).\n\n**Explanation:**\n- Each observation has one residual: uᵢ = yᵢ - ŷᵢ\n- It's the difference between actual and predicted value\n\n**Not** size p (number of features)!"
    },
    {
      "id": 133,
      "question": "Why would you look at partial regression plots in multivariate regression?",
      "answer": "To visualize the effect of a **particular feature** on the dependent variable **while holding all other features constant**.\n\n**Use cases:**\n- Detect non-linear relationships\n- Identify outliers affecting specific features\n- Understand individual feature contributions\n- Check linearity assumption for each predictor"
    },
    {
      "id": 134,
      "question": "How do you check for collinearity in regression?",
      "answer": "**Methods:**\n\n1. **Correlation matrix**: Shows pairwise correlations between features\n\n2. **Rank of feature matrix**: If rank < number of features → multicollinearity\n\n3. **VIF (Variance Inflation Factor)**:\n   - VIF > 5-10 indicates problematic collinearity\n   - Detects multicollinearity beyond just pairs\n\n**Signs:** Large standard errors + high R² but few significant coefficients"
    },
    {
      "id": 135,
      "question": "How many solutions to OLS exist if two features are collinear?",
      "answer": "**No single solution** exists if two features are perfectly collinear.\n\n**Why?**\n- The feature matrix is not full rank\n- Matrix is not invertible\n- Infinite combinations of coefficients produce the same predictions\n\n**Solution:** Remove one of the collinear features, or use regularization (Ridge regression)"
    },
    {
      "id": 136,
      "question": "What is the formula of a log-odd (logit) for a probability p?",
      "answer": "**Formula:** logit(p) = log(p / (1-p))\n\n**Explanation:**\n- p/(1-p) is the **odds ratio** (probability of success / probability of failure)\n- The log transforms odds from [0, ∞) to (-∞, +∞)\n\n**In logistic regression:** logit(p) = β₀ + β₁X₁ + ... + βₙXₙ"
    },
    {
      "id": 137,
      "question": "What interval does the logit transformation map probability [0,1] into?",
      "answer": "The logit transformation maps probability range **[0, 1]** to log-odds **(-∞, +∞)**.\n\n**Mapping:**\n- p = 0.5 → logit = 0\n- p < 0.5 → logit < 0\n- p > 0.5 → logit > 0\n- p → 0 → logit → -∞\n- p → 1 → logit → +∞"
    },
    {
      "id": 138,
      "question": "Given probability of success = 0.8, what are the odds of success?",
      "answer": "**Calculation:**\n- Odds ratio = p / (1-p) = 0.8 / 0.2 = 4\n\n**Interpretation:** The odds of success are **4 to 1**.\n\nThis means success is 4 times more likely than failure."
    },
    {
      "id": 139,
      "question": "How do you interpret log-odds coefficients in logistic regression?",
      "answer": "**Example output:**\n```\nIntercept: 0.53\nC(pclass)[T.2]: -0.64\n```\n\n**Interpretation:**\n- 0.53 = log-odds of survival for first class passengers (reference category)\n- -0.64 = **decrease** in log-odds for 2nd class **relative to** 1st class\n\n**If log-odds < 0:** probability < 50%\n**If log-odds > 0:** probability > 50%"
    },
    {
      "id": 140,
      "question": "Is R² a good measure for logistic regression?",
      "answer": "**No!** In logistic regression, we can't calculate R² directly. Instead, we use **pseudo-R²** (like McFadden's).\n\n**Limitations of pseudo-R²:**\n- **Valid** for comparing models on the **same dataset**\n- **Invalid** for comparing models on **different datasets**\n- Not as interpretable as linear regression R²\n\n**Better metrics:** AUC-ROC, accuracy, precision, recall, F1-score"
    },
    {
      "id": 141,
      "question": "What is the rank of a matrix? When is a matrix invertible?",
      "answer": "**Rank:** The dimension of the vector-space formed by the matrix's columns (number of linearly independent columns).\n\n**Invertibility condition:**\nMatrix is invertible **if and only if** rank = number of columns (full rank).\n\n**Implications for regression:**\n- Full rank → unique OLS solution\n- Not full rank → collinearity → infinite solutions or no solution"
    },
    {
      "id": 142,
      "question": "What is the difference between lemmatization and stemming?",
      "answer": "**Stemming:**\n- Cuts off word endings/beginnings using rules\n- Faster but less accurate\n- May produce non-words (\"studies\" → \"studi\")\n- Example: Porter Stemmer\n\n**Lemmatization:**\n- Uses morphological analysis and vocabulary\n- Returns actual dictionary words (\"studies\" → \"study\")\n- Slower but more accurate\n- Example: WordNet Lemmatizer"
    },
    {
      "id": 143,
      "question": "What does LDA stand for and what is it used for?",
      "answer": "**LDA = Latent Dirichlet Allocation**\n\nA generative statistical model for **topic modeling**.\n\n**How it works:**\n- Assumes documents are mixtures of topics\n- Topics are mixtures of words\n- Discovers hidden topics from word distributions\n\n**Use cases:**\n- Document classification\n- Content recommendation\n- Understanding corpus themes"
    },
    {
      "id": 144,
      "question": "What are POS tags?",
      "answer": "**POS = Part of Speech tags**\n\nTags attributed to each word specifying its **grammatical role** in the sentence.\n\n**Examples:**\n- NN: Noun\n- VB: Verb\n- JJ: Adjective\n- RB: Adverb\n\n**Use cases:**\n- Improve lemmatization (WordNetLemmatizer uses POS)\n- Named Entity Recognition\n- Information extraction\n- Feature engineering for NLP models"
    },
    {
      "id": 145,
      "question": "How do you find strings matching a pattern in Python?",
      "answer": "Use the **re.search()** method from the regex module:\n\n```python\nimport re\nmatch = re.search(pattern, string)\n```\n\n**Other useful methods:**\n- `re.match()`: Match at beginning only\n- `re.findall()`: Find all occurrences\n- `re.sub()`: Find and replace"
    },
    {
      "id": 146,
      "question": "What is a neuron in an Artificial Neural Network?",
      "answer": "A neuron is a mathematical function:\n\n**Structure:** Linear combination of inputs + bias, wrapped in an activation function.\n\n**Formula:** output = activation(Σ(wᵢxᵢ) + b)\n\n**Analogy:** Modeled after biological neurons, which receive signals, process them, and fire (or not) based on the combined input."
    },
    {
      "id": 147,
      "question": "What are activation functions and why do we need them?",
      "answer": "**Activation functions** are mathematical equations that determine whether a neuron should be activated based on its input.\n\n**Why needed:**\nWithout activation functions, a neural network would just be a linear combination of linear combinations = still linear!\n\n**Common functions:**\n- ReLU: max(0, x)\n- Sigmoid: 1/(1+e⁻ˣ)\n- Tanh: (eˣ-e⁻ˣ)/(eˣ+e⁻ˣ)\n- Softmax: for multi-class output"
    },
    {
      "id": 148,
      "question": "What are the most common loss functions for classification?",
      "answer": "**Binary Cross-Entropy:**\n- For binary classification (2 classes)\n- L = -[y·log(p) + (1-y)·log(1-p)]\n\n**Categorical Cross-Entropy:**\n- For multi-class classification\n- L = -Σ yᵢ·log(pᵢ)\n\n**Sparse Categorical Cross-Entropy:**\n- Same as categorical but with integer labels instead of one-hot"
    },
    {
      "id": 149,
      "question": "What activation function should you use for multi-class classification?",
      "answer": "**Softmax** activation in the output layer.\n\n**Properties:**\n- Outputs n probabilities (one per class)\n- All outputs are between 0 and 1\n- All outputs sum to 1\n\n**Formula:** softmax(xᵢ) = e^xᵢ / Σe^xⱼ"
    },
    {
      "id": 150,
      "question": "What is the role of a Dropout layer?",
      "answer": "Dropout **temporarily cancels** the contribution of some neurons during training.\n\n**Purpose:**\n- Prevents neurons from over-specializing (co-adaptation)\n- Acts as regularization to reduce overfitting\n- Forces the network to learn more robust features\n\n**Note:** Dropout is only active during training, not inference."
    },
    {
      "id": 151,
      "question": "If you want parameters updated more frequently, should you increase or decrease batch size?",
      "answer": "**Decrease** the batch size.\n\n**Why?**\n- Parameters update once per batch\n- Smaller batches = more updates per epoch\n- More frequent updates can help with convergence\n\n**Trade-off:**\n- Smaller batch: noisier gradients, more updates\n- Larger batch: smoother gradients, fewer updates, better GPU utilization"
    },
    {
      "id": 152,
      "question": "Explain forward propagation and backward propagation.",
      "answer": "**Forward Propagation:**\n1. Input batch goes through network layers\n2. Each layer applies: output = activation(weights·input + bias)\n3. Final layer produces predictions\n\n**Loss Computation:**\n- Compare predictions to true values\n\n**Backward Propagation:**\n1. Compute gradient of loss w.r.t. each weight (chain rule)\n2. Start from output layer, go backwards\n3. Update weights: w = w - learning_rate × gradient"
    },
    {
      "id": 153,
      "question": "Why is it necessary to compile a Neural Network? What are the 3 components?",
      "answer": "**Compiling** tells Keras how to optimize the network.\n\n**Three components:**\n\n1. **loss**: Function to minimize DURING training\n   (e.g., 'binary_crossentropy', 'mse')\n\n2. **optimizer**: Algorithm for weight updates\n   (e.g., 'adam', 'sgd', 'rmsprop')\n\n3. **metrics**: How to evaluate AFTER training\n   (e.g., 'accuracy', 'precision')"
    },
    {
      "id": 154,
      "question": "How do you normalize images for CNN training?",
      "answer": "**Standard approach:**\n\nDivide pixel values by 255:\n```python\nX_normalized = X / 255.0\n```\n\n**Why?**\n- RGB images have pixel values 0-255\n- Neural networks work better with values in [0, 1] or [-1, 1]\n- Helps with gradient flow and convergence\n\n**Alternative:** Standardization (subtract mean, divide by std)"
    },
    {
      "id": 155,
      "question": "What is the difference between kernels and filters in CNNs?",
      "answer": "**Kernel:**\n- A small 2D matrix (e.g., 3×3)\n- Scans ONE channel of the input\n- Extracts spatial patterns\n\n**Filter:**\n- Contains **multiple kernels** (one per input channel)\n- For RGB image: 1 filter = 3 kernels (one per color)\n- Produces ONE output channel\n\n**Example:** Conv2D with 32 filters on RGB input:\n32 filters × 3 kernels each = 96 kernels total"
    },
    {
      "id": 156,
      "question": "When should you use Data Augmentation with images?",
      "answer": "**Use Data Augmentation when:**\n\n1. **Dataset is highly unbalanced** - Generate more samples for minority classes\n\n2. **Neural net is overfitting** - More variety helps generalization\n\n3. **Real-life photos** (animals, objects) - Rotations, flips, zooms create valid variants\n\n**NOT useful when:**\n- Neural net is underfitting (need more capacity, not more data variation)"
    },
    {
      "id": 157,
      "question": "Why use CNN instead of a Dense network for images?",
      "answer": "**Problems with Dense networks on images:**\n1. Too many parameters (flattened 224×224×3 = 150K inputs!)\n2. No spatial awareness\n\n**CNN advantages:**\n1. **Translation invariance**: Recognizes objects regardless of position\n2. **Parameter sharing**: Same kernel applied everywhere\n3. **Hierarchical features**: Edges → Shapes → Objects\n\n**Key concept:** Images convey same info even if rotated, cropped, or mirrored"
    },
    {
      "id": 158,
      "question": "What do the dimensions (None, 32, 32, 128) mean in a CNN layer output?",
      "answer": "For output shape **(None, 32, 32, 128)**:\n\n- **None**: Batch size (placeholder, varies at runtime)\n- **32 × 32**: Spatial dimensions (height × width)\n- **128**: Number of channels/feature maps\n\n**Note:** In early layers, 3 channels = RGB colors. In deeper layers, channels represent learned features, not colors."
    },
    {
      "id": 159,
      "question": "What is an Autoencoder and what is it used for?",
      "answer": "An **Autoencoder** is a neural network with two parts:\n\n**Encoder:** Compresses input (e.g., 224×224×3 → 8 values)\n**Decoder:** Reconstructs original from compressed representation\n\n**Training:** `autoencoder.fit(X_train, X_train)` - reconstruct input!\n\n**Use cases:**\n- **Compression**: Find minimal latent representation\n- **Feature learning**: Use encoder output as features\n- **Generative models**: Sample from latent space, decode to create new data"
    },
    {
      "id": 160,
      "question": "Name two architectures of Recurrent layers.",
      "answer": "1. **LSTM (Long Short-Term Memory)**\n   - Uses gates (forget, input, output) to control information flow\n   - Can learn long-term dependencies\n   - Solves vanishing gradient problem\n\n2. **GRU (Gated Recurrent Unit)**\n   - Simplified version of LSTM\n   - Uses reset and update gates\n   - Fewer parameters, faster to train\n   - Often similar performance to LSTM"
    },
    {
      "id": 161,
      "question": "What is the basic idea behind Transformer models?",
      "answer": "Transformers use **attention mechanisms** to process sequences.\n\n**Key innovations:**\n- Process entire sequence **in parallel** (not sequentially like RNNs)\n- Handle **long-range dependencies** effectively\n- \"Attention is all you need\" - no recurrence or convolution\n\n**Result:** Much faster training, better at capturing relationships between distant words."
    },
    {
      "id": 162,
      "question": "What differentiates Transformers from RNNs?",
      "answer": "**RNNs:**\n- Process sequences **step by step** (sequential)\n- Hidden state carries information forward\n- Struggle with long sequences (vanishing gradient)\n- Slow to train (can't parallelize)\n\n**Transformers:**\n- Process entire sequence **in parallel**\n- Use attention to relate any two positions directly\n- Handle long sequences well\n- Much faster training on GPUs"
    },
    {
      "id": 163,
      "question": "What is the difference between self-attention and cross-attention?",
      "answer": "**Self-attention:**\n- Parts of input interact **with each other**\n- Used in Encoders\n- Example: Understanding how words in a sentence relate to each other\n\n**Cross-attention:**\n- Decoder attends to **Encoder outputs**\n- Integrates source context into target generation\n- Example: In translation, aligning target words with relevant source words"
    },
    {
      "id": 164,
      "question": "What is GPT and how does it work?",
      "answer": "**GPT = Generative Pre-trained Transformer**\n\n**Architecture:**\n- Decoder-only Transformer\n- Autoregressive: predicts next token based on previous tokens\n\n**Training:**\n1. Pre-training: Predict next word on huge corpus\n2. Fine-tuning: Adapt to specific tasks\n\n**Use:** Text generation based on prompts"
    },
    {
      "id": 165,
      "question": "What is BERT and what is it used for?",
      "answer": "**BERT = Bidirectional Encoder Representations from Transformers**\n\n**Architecture:**\n- Encoder-only Transformer\n- Bidirectional: considers both left and right context\n\n**Pre-training tasks:**\n- Masked Language Model (MLM): predict masked words\n- Next Sentence Prediction (NSP)\n\n**Use cases:**\n- Text classification\n- Question answering\n- Named Entity Recognition\n- Sentiment analysis"
    },
    {
      "id": 166,
      "question": "What is the attention mechanism in Transformers?",
      "answer": "**Attention** lets the model focus on relevant parts of the input.\n\n**Components:**\n- **Query (Q)**: What we're looking for\n- **Key (K)**: What we're matching against\n- **Value (V)**: The actual information\n\n**Formula:**\nAttention(Q, K, V) = softmax(QKᵀ / √dₖ) × V\n\n**Multi-head attention:** Multiple attention layers in parallel, each learning different relationships."
    },
    {
      "id": 167,
      "question": "How can you check if a Python package is installed?",
      "answer": "**Using pip:**\n\n```bash\n# List all installed packages\npip freeze\n\n# Check for specific package\npip freeze | grep package-name\n\n# Alternative\npip show package-name\n```\n\n**In Python:**\n```python\nimport pkg_resources\npkg_resources.get_distribution('package-name').version\n```"
    },
    {
      "id": 168,
      "question": "What is the difference between pip freeze and pip list?",
      "answer": "**pip freeze:**\n- Output format suitable for requirements.txt\n- Shows package==version\n- Used for reproducibility\n\n**pip list:**\n- Human-readable table format\n- Shows Package and Version columns\n- Better for quick inspection\n\n**Best practice:** Use `pip freeze > requirements.txt` to capture dependencies"
    },
    {
      "id": 169,
      "question": "What is Docker and why is it useful for ML?",
      "answer": "**Docker** creates containerized, reproducible environments.\n\n**Benefits for ML:**\n1. **Reproducibility**: Same environment everywhere\n2. **Dependency management**: Avoid \"works on my machine\"\n3. **Deployment**: Package model + code + dependencies\n4. **Scalability**: Easy to replicate containers\n\n**Key concepts:**\n- Dockerfile: Recipe for building image\n- Image: Blueprint for container\n- Container: Running instance"
    },
    {
      "id": 170,
      "question": "What is the purpose of a requirements.txt file?",
      "answer": "**requirements.txt** lists all Python dependencies for a project.\n\n**Purpose:**\n- Reproducible environments\n- Easy installation: `pip install -r requirements.txt`\n- Version pinning for stability\n\n**Best practices:**\n- Pin exact versions for production: `numpy==1.21.0`\n- Use ranges for flexibility: `numpy>=1.20,<2.0`\n- Generate with: `pip freeze > requirements.txt`"
    },
    {
      "id": 201,
      "question": "What is a PyTorch tensor?",
      "answer": "A **tensor** is a multi-dimensional array, similar to NumPy arrays but with GPU acceleration support.\n\n**Key features:**\n- Can run on GPU for faster computation\n- Automatic differentiation support (autograd)\n- Building block for all PyTorch operations\n\n**Creation:**\n```python\nimport torch\nx = torch.tensor([[1, 2], [3, 4]])\n```"
    },
    {
      "id": 202,
      "question": "How do you move a tensor to GPU in PyTorch?",
      "answer": "**Method 1: Using .to()**\n```python\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ntensor_gpu = tensor.to(device)\n```\n\n**Method 2: Using .cuda()**\n```python\ntensor_gpu = tensor.cuda()\n```\n\n**Best practice:** Always check availability first to avoid errors on CPU-only machines."
    },
    {
      "id": 203,
      "question": "What is the difference between torch.tensor() and torch.Tensor()?",
      "answer": "**torch.tensor()** (lowercase):\n- Factory function\n- Infers dtype from data\n- Recommended approach\n- More flexible\n\n**torch.Tensor()** (uppercase):\n- Constructor for default tensor type\n- Always creates FloatTensor\n- Legacy approach\n\n```python\ntorch.tensor([1, 2])  # IntTensor\ntorch.Tensor([1, 2])  # FloatTensor\n```"
    },
    {
      "id": 204,
      "question": "What is autograd in PyTorch?",
      "answer": "**Autograd** is PyTorch's automatic differentiation engine.\n\n**How it works:**\n- Tracks all operations on tensors with `requires_grad=True`\n- Builds a computational graph dynamically\n- Computes gradients automatically via backpropagation\n\n```python\nx = torch.tensor([2.0], requires_grad=True)\ny = x ** 2\ny.backward()  # Compute dy/dx\nprint(x.grad)  # tensor([4.])\n```"
    },
    {
      "id": 205,
      "question": "What is the purpose of requires_grad in PyTorch?",
      "answer": "**requires_grad=True** tells PyTorch to track operations for gradient computation.\n\n**When to use:**\n- Set to True for model parameters (weights, biases)\n- Set to False for data tensors\n- Automatically True for `nn.Parameter`\n\n**Performance:**\n- Tracking gradients uses memory\n- Disable during inference: `with torch.no_grad():`"
    },
    {
      "id": 206,
      "question": "How do you define a simple neural network in PyTorch?",
      "answer": "```python\nimport torch.nn as nn\n\nclass SimpleNet(nn.Module):\n    def __init__(self):\n        super(SimpleNet, self).__init__()\n        self.fc1 = nn.Linear(784, 128)\n        self.fc2 = nn.Linear(128, 10)\n    \n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\nmodel = SimpleNet()\n```\n\n**Key components:**\n- Inherit from `nn.Module`\n- Define layers in `__init__`\n- Define forward pass in `forward`"
    },
    {
      "id": 207,
      "question": "What is the difference between nn.Sequential and nn.Module?",
      "answer": "**nn.Sequential:**\n- Quick way to stack layers linearly\n- No need to define forward()\n- Limited to sequential architectures\n\n```python\nmodel = nn.Sequential(\n    nn.Linear(784, 128),\n    nn.ReLU(),\n    nn.Linear(128, 10)\n)\n```\n\n**nn.Module:**\n- Full control over architecture\n- Can implement complex forward logic\n- Required for skip connections, branching, etc."
    },
    {
      "id": 208,
      "question": "How do you create a DataLoader in PyTorch?",
      "answer": "```python\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Create dataset\ndataset = TensorDataset(X_train, y_train)\n\n# Create dataloader\ntrain_loader = DataLoader(\n    dataset,\n    batch_size=32,\n    shuffle=True,\n    num_workers=4\n)\n```\n\n**Parameters:**\n- `batch_size`: Samples per batch\n- `shuffle`: Randomize order\n- `num_workers`: Parallel data loading"
    },
    {
      "id": 209,
      "question": "What are the typical steps in a PyTorch training loop?",
      "answer": "```python\nfor epoch in range(num_epochs):\n    for batch_X, batch_y in train_loader:\n        # 1. Forward pass\n        outputs = model(batch_X)\n        loss = criterion(outputs, batch_y)\n        \n        # 2. Zero gradients\n        optimizer.zero_grad()\n        \n        # 3. Backward pass\n        loss.backward()\n        \n        # 4. Update weights\n        optimizer.step()\n```\n\n**Critical:** Always zero gradients before backward() to avoid accumulation!"
    },
    {
      "id": 210,
      "question": "Why must you call optimizer.zero_grad() in PyTorch?",
      "answer": "PyTorch **accumulates gradients** by default - each `.backward()` call adds to existing gradients.\n\n**Without zero_grad():**\n- Gradients from multiple batches sum up\n- Leads to incorrect weight updates\n- Model won't train properly\n\n**Use case for accumulation:**\nGradient accumulation for large effective batch sizes:\n```python\nfor i, (X, y) in enumerate(loader):\n    loss = criterion(model(X), y)\n    loss.backward()\n    if (i+1) % accumulation_steps == 0:\n        optimizer.step()\n        optimizer.zero_grad()\n```"
    },
    {
      "id": 211,
      "question": "What is the difference between model.eval() and torch.no_grad()?",
      "answer": "**model.eval():**\n- Changes model **behavior** (disables dropout, uses running stats for BatchNorm)\n- Does NOT disable gradient computation\n- Affects layers like Dropout, BatchNorm\n\n**torch.no_grad():**\n- Disables gradient computation (saves memory)\n- Does NOT change model behavior\n- Used during inference for efficiency\n\n**Best practice for inference:**\n```python\nmodel.eval()\nwith torch.no_grad():\n    predictions = model(X_test)\n```"
    },
    {
      "id": 212,
      "question": "How do you save and load a PyTorch model?",
      "answer": "**Save:**\n```python\n# Save entire model (not recommended)\ntorch.save(model, 'model.pth')\n\n# Save state dict (recommended)\ntorch.save(model.state_dict(), 'model_weights.pth')\n\n# Save checkpoint with optimizer\ntorch.save({\n    'model_state_dict': model.state_dict(),\n    'optimizer_state_dict': optimizer.state_dict(),\n    'epoch': epoch\n}, 'checkpoint.pth')\n```\n\n**Load:**\n```python\nmodel = TheModelClass()\nmodel.load_state_dict(torch.load('model_weights.pth'))\nmodel.eval()\n```"
    },
    {
      "id": 213,
      "question": "What are common PyTorch optimizers and when to use them?",
      "answer": "**SGD (Stochastic Gradient Descent):**\n- Simple, reliable baseline\n- Needs careful learning rate tuning\n- Add momentum for better convergence\n\n**Adam:**\n- Adaptive learning rates per parameter\n- Works well with default settings\n- Most popular choice\n\n**AdamW:**\n- Adam with decoupled weight decay\n- Better for transformer models\n\n**RMSprop:**\n- Good for RNNs\n- Adapts learning rates\n\n```python\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n```"
    },
    {
      "id": 214,
      "question": "What is a learning rate scheduler in PyTorch?",
      "answer": "A **scheduler** adjusts the learning rate during training.\n\n**Common schedulers:**\n\n```python\n# Reduce LR when metric plateaus\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer, mode='min', patience=5\n)\n\n# Step decay\nscheduler = StepLR(optimizer, step_size=30, gamma=0.1)\n\n# Cosine annealing\nscheduler = CosineAnnealingLR(optimizer, T_max=100)\n```\n\n**Usage:**\n```python\nfor epoch in range(num_epochs):\n    train(...)\n    scheduler.step()  # Update learning rate\n```"
    },
    {
      "id": 215,
      "question": "How do you implement custom loss functions in PyTorch?",
      "answer": "**Method 1: Simple function**\n```python\ndef custom_loss(output, target):\n    loss = torch.mean((output - target) ** 2)\n    return loss\n```\n\n**Method 2: Class (for complex losses)**\n```python\nclass CustomLoss(nn.Module):\n    def __init__(self, weight=1.0):\n        super().__init__()\n        self.weight = weight\n    \n    def forward(self, output, target):\n        loss = self.weight * torch.mean((output - target) ** 2)\n        return loss\n\ncriterion = CustomLoss(weight=0.5)\n```"
    },
    {
      "id": 216,
      "question": "What is the purpose of nn.Embedding in PyTorch?",
      "answer": "**nn.Embedding** creates learnable dense vector representations for categorical data (like words).\n\n```python\n# vocab_size=1000, embedding_dim=128\nembedding = nn.Embedding(1000, 128)\n\n# Input: tensor of indices [2, 5, 8]\ninput_ids = torch.LongTensor([2, 5, 8])\nembedded = embedding(input_ids)  # Shape: [3, 128]\n```\n\n**Use cases:**\n- Word embeddings in NLP\n- User/item embeddings in recommender systems\n- Categorical feature encoding"
    },
    {
      "id": 217,
      "question": "How do you freeze layers in PyTorch?",
      "answer": "**Freeze specific layers:**\n```python\n# Freeze all parameters\nfor param in model.parameters():\n    param.requires_grad = False\n\n# Unfreeze last layer\nfor param in model.fc.parameters():\n    param.requires_grad = True\n```\n\n**Why freeze?**\n- Transfer learning: keep pre-trained features\n- Fine-tuning: train only classification head\n- Reduce computation\n\n**Important:** Only pass unfrozen params to optimizer:\n```python\noptimizer = torch.optim.Adam(\n    filter(lambda p: p.requires_grad, model.parameters())\n)\n```"
    },
    {
      "id": 218,
      "question": "What is the difference between view() and reshape() in PyTorch?",
      "answer": "**view():**\n- Returns a view when possible (shares memory)\n- Requires tensor to be **contiguous**\n- Fails if tensor is not contiguous\n\n**reshape():**\n- Returns view when possible, copy otherwise\n- Works on non-contiguous tensors\n- More flexible, slightly slower\n\n```python\nx = torch.randn(4, 4)\ny = x.transpose(0, 1)  # Now non-contiguous\n\ny.view(16)     # Error!\ny.reshape(16)  # Works (makes copy)\n```\n\n**Best practice:** Use reshape() unless you specifically need view()"
    },
    {
      "id": 219,
      "question": "How do you implement gradient clipping in PyTorch?",
      "answer": "**Gradient clipping** prevents exploding gradients by limiting gradient magnitude.\n\n```python\nfor batch in train_loader:\n    optimizer.zero_grad()\n    output = model(batch)\n    loss = criterion(output, target)\n    loss.backward()\n    \n    # Clip gradients\n    torch.nn.utils.clip_grad_norm_(\n        model.parameters(), \n        max_norm=1.0\n    )\n    \n    optimizer.step()\n```\n\n**When to use:**\n- Training RNNs/LSTMs\n- Unstable training\n- Very deep networks"
    },
    {
      "id": 220,
      "question": "What is mixed precision training in PyTorch?",
      "answer": "**Mixed precision** uses float16 for speed, float32 for stability.\n\n```python\nfrom torch.cuda.amp import autocast, GradScaler\n\nscaler = GradScaler()\n\nfor batch in train_loader:\n    optimizer.zero_grad()\n    \n    # Forward in float16\n    with autocast():\n        output = model(batch)\n        loss = criterion(output, target)\n    \n    # Backward with gradient scaling\n    scaler.scale(loss).backward()\n    scaler.step(optimizer)\n    scaler.update()\n```\n\n**Benefits:**\n- ~2x faster training\n- ~50% less memory\n- Requires Tensor Cores (modern GPUs)"
    },
    {
      "id": 221,
      "question": "What is a graph in the context of GNNs?",
      "answer": "A **graph** G = (V, E) consists of:\n\n- **V (Vertices/Nodes)**: Entities (users, molecules, papers)\n- **E (Edges)**: Relationships between nodes\n\n**Types:**\n- **Undirected**: Friendship (symmetric)\n- **Directed**: Follow relationship (asymmetric)\n- **Weighted**: Edges have values (distance, similarity)\n\n**Representations:**\n- Adjacency matrix A: A[i,j] = 1 if edge exists\n- Edge list: [(0,1), (1,2), ...]"
    },
    {
      "id": 222,
      "question": "Why can't we use standard CNNs on graphs?",
      "answer": "**Problems with graphs:**\n\n1. **No fixed structure**: Number of neighbors varies per node\n2. **No spatial locality**: No grid-like structure\n3. **Permutation invariance**: Node ordering shouldn't matter\n4. **Variable size**: Graphs have different numbers of nodes\n\n**CNNs assume:**\n- Fixed grid structure (images are always H×W)\n- Local connectivity (3×3 kernels)\n- Translation invariance\n\n**Solution:** GNNs use message passing instead of convolutions"
    },
    {
      "id": 223,
      "question": "What is the message passing framework in GNNs?",
      "answer": "**Message passing** is the core mechanism of GNNs.\n\n**Three steps per layer:**\n\n1. **Message**: Each node creates messages for neighbors\n   `mᵢⱼ = MSG(hᵢ, hⱼ, eᵢⱼ)`\n\n2. **Aggregate**: Collect messages from neighbors\n   `aᵢ = AGG({mⱼᵢ : j ∈ N(i)})`\n\n3. **Update**: Compute new node representation\n   `hᵢ' = UPDATE(hᵢ, aᵢ)`\n\n**Common aggregators:** SUM, MEAN, MAX, attention-weighted"
    },
    {
      "id": 224,
      "question": "What are node features, edge features, and graph features?",
      "answer": "**Node features (h):**\n- Properties of individual nodes\n- Examples: atom type, user age, word embedding\n- Shape: [num_nodes, node_feature_dim]\n\n**Edge features (e):**\n- Properties of relationships\n- Examples: bond type, friendship duration, edge weight\n- Shape: [num_edges, edge_feature_dim]\n\n**Graph features (u):**\n- Properties of entire graph\n- Examples: molecule properties, graph metadata\n- Computed via global pooling over nodes"
    },
    {
      "id": 225,
      "question": "What is a Graph Convolutional Network (GCN)?",
      "answer": "**GCN** is a popular GNN architecture that performs spectral convolution on graphs.\n\n**Layer formula:**\n```\nH^(l+1) = σ(D̃^(-1/2) Ã D̃^(-1/2) H^(l) W^(l))\n```\n\nWhere:\n- Ã = A + I (adjacency + self-loops)\n- D̃ = degree matrix of Ã\n- W = learnable weight matrix\n- σ = activation function\n\n**Intuition:** Average neighbor features, then apply linear transformation\n\n**PyTorch Geometric:**\n```python\nfrom torch_geometric.nn import GCNConv\nconv = GCNConv(in_channels, out_channels)\n```"
    },
    {
      "id": 226,
      "question": "What is over-smoothing in GNNs?",
      "answer": "**Over-smoothing**: As GNN layers increase, node representations become **too similar**.\n\n**Cause:**\n- Each layer aggregates from neighbors\n- After k layers, nodes receive info from k-hop neighbors\n- Deep GNNs → all nodes have similar representations\n\n**Consequences:**\n- Loss of node-specific information\n- Poor performance on node classification\n\n**Solutions:**\n- Limit depth (2-3 layers often enough)\n- Skip connections (like ResNet)\n- Jumping Knowledge Networks\n- Graph normalization techniques"
    },
    {
      "id": 227,
      "question": "What are the three main types of graph learning tasks?",
      "answer": "**1. Node-level tasks:**\n- Predict properties of individual nodes\n- Example: Node classification (user interests)\n- Output: One prediction per node\n\n**2. Edge-level tasks:**\n- Predict properties of edges or edge existence\n- Example: Link prediction (friend recommendation)\n- Output: One prediction per edge (or node pair)\n\n**3. Graph-level tasks:**\n- Predict properties of entire graphs\n- Example: Molecular property prediction\n- Output: One prediction per graph\n- Requires **graph pooling** layer"
    },
    {
      "id": 228,
      "question": "What is graph pooling and why is it needed?",
      "answer": "**Graph pooling** aggregates node representations into a single graph-level representation.\n\n**When needed:**\n- Graph classification tasks\n- Graph regression tasks\n\n**Common methods:**\n\n**Global pooling:**\n```python\n# Mean pooling\nh_graph = torch.mean(node_features, dim=0)\n\n# Max pooling\nh_graph = torch.max(node_features, dim=0)\n\n# Sum pooling\nh_graph = torch.sum(node_features, dim=0)\n```\n\n**Learnable pooling:**\n- DiffPool\n- TopKPooling\n- SAGPool"
    },
    {
      "id": 229,
      "question": "What is PyTorch Geometric (PyG)?",
      "answer": "**PyTorch Geometric** is a library for deep learning on graphs.\n\n**Key features:**\n- Built on PyTorch\n- Efficient sparse tensor operations\n- Many pre-implemented GNN layers\n- Graph data structures and utilities\n- Common graph datasets\n\n**Basic usage:**\n```python\nimport torch\nfrom torch_geometric.data import Data\n\nedge_index = torch.tensor([[0, 1], [1, 2]], dtype=torch.long).t()\nx = torch.tensor([[1], [2], [3]], dtype=torch.float)\n\ndata = Data(x=x, edge_index=edge_index)\n```"
    },
    {
      "id": 230,
      "question": "What is the edge_index format in PyTorch Geometric?",
      "answer": "**edge_index** is a [2, num_edges] tensor representing edges.\n\n**Format:**\n```python\nedge_index = torch.tensor([\n    [source_nodes],  # Row 0: source nodes\n    [target_nodes]   # Row 1: target nodes\n], dtype=torch.long)\n```\n\n**Example:**\n```python\n# Edges: 0→1, 1→2, 2→0\nedge_index = torch.tensor([\n    [0, 1, 2],  # Source\n    [1, 2, 0]   # Target\n], dtype=torch.long)\n```\n\n**For undirected graphs:** Include both directions:\n```python\nedge_index = torch.tensor([\n    [0, 1, 1, 2, 2, 0],\n    [1, 0, 2, 1, 0, 2]\n])\n```"
    },
    {
      "id": 231,
      "question": "What is GraphSAGE and how does it differ from GCN?",
      "answer": "**GraphSAGE (SAmple and aggreGatE)** samples a fixed number of neighbors instead of using all.\n\n**Key differences from GCN:**\n\n**GCN:**\n- Uses ALL neighbors\n- Weighted by degree normalization\n- Can be slow on large graphs\n\n**GraphSAGE:**\n- **Samples** K neighbors (e.g., K=10)\n- Scalable to huge graphs\n- Multiple aggregators: MEAN, LSTM, Pool\n\n```python\nfrom torch_geometric.nn import SAGEConv\n\nconv = SAGEConv(in_channels, out_channels)\n```\n\n**Benefits:** Inductive learning (can generalize to unseen nodes)"
    },
    {
      "id": 232,
      "question": "What is Graph Attention Network (GAT)?",
      "answer": "**GAT** uses attention mechanisms to weight neighbor contributions.\n\n**Key idea:**\nInstead of averaging neighbors equally, learn which neighbors are more important.\n\n**Attention weights:**\n```\nαᵢⱼ = softmax(LeakyReLU(a^T [Whᵢ || Whⱼ]))\n```\n\n**Update:**\n```\nhᵢ' = σ(Σⱼ αᵢⱼ Whⱼ)\n```\n\n**Advantages:**\n- Automatically learns neighbor importance\n- Different attention weights per neighbor\n- Multi-head attention for robustness\n\n```python\nfrom torch_geometric.nn import GATConv\n\nconv = GATConv(in_channels, out_channels, heads=8)\n```"
    },
    {
      "id": 233,
      "question": "What is the difference between transductive and inductive learning in GNNs?",
      "answer": "**Transductive learning:**\n- Training and test nodes are from the **same graph**\n- All nodes (train/val/test) visible during training\n- Only labels are hidden for test nodes\n- Example: Semi-supervised node classification\n\n**Inductive learning:**\n- Model must generalize to **completely new graphs**\n- Test graphs unseen during training\n- More challenging and realistic\n- Example: Molecular property prediction\n\n**Models:**\n- GCN: Primarily transductive\n- GraphSAGE: Designed for inductive\n- GAT: Can do both"
    },
    {
      "id": 234,
      "question": "What is a heterogeneous graph?",
      "answer": "A **heterogeneous graph** has multiple types of nodes and/or edges.\n\n**Example: Academic network**\n- Node types: Papers, Authors, Venues\n- Edge types: writes (Author→Paper), published_at (Paper→Venue), cites (Paper→Paper)\n\n**Contrast with homogeneous:**\n- Homogeneous: Single node type, single edge type\n- Heterogeneous: Multiple types\n\n**Handling:**\n```python\nfrom torch_geometric.nn import HeteroConv\n\n# Different message passing per edge type\nconv = HeteroConv({\n    ('paper', 'cites', 'paper'): GCNConv(...),\n    ('author', 'writes', 'paper'): SAGEConv(...),\n})\n```"
    },
    {
      "id": 235,
      "question": "What is link prediction in GNNs and how do you implement it?",
      "answer": "**Link prediction**: Predict whether an edge exists between two nodes.\n\n**Approach:**\n\n1. **Train GNN** to learn node embeddings\n2. **Compute edge scores** from node pairs:\n   - Dot product: `score = hᵢ · hⱼ`\n   - Concatenation + MLP: `score = MLP([hᵢ || hⱼ])`\n   - Distance-based: `score = -||hᵢ - hⱼ||`\n\n3. **Negative sampling**: Sample non-edges as negative examples\n\n```python\n# Get embeddings\nh = gnn(x, edge_index)\n\n# Edge score\nscore = (h[edge_index[0]] * h[edge_index[1]]).sum(dim=-1)\n\n# Loss\nloss = criterion(score, edge_labels)\n```"
    },
    {
      "id": 236,
      "question": "What is the receptive field of a GNN?",
      "answer": "The **receptive field** is the set of nodes that can influence a target node's representation.\n\n**Depth relationship:**\n- 1-layer GNN: 1-hop neighbors\n- 2-layer GNN: 2-hop neighbors\n- k-layer GNN: k-hop neighbors\n\n**Example:**\n```\nGraph: 0 -- 1 -- 2 -- 3\n\nNode 3's receptive field:\n- 1 layer: {2}\n- 2 layers: {1, 2}\n- 3 layers: {0, 1, 2}\n```\n\n**Trade-off:**\n- Deeper → larger receptive field → more context\n- But: over-smoothing, computational cost"
    },
    {
      "id": 237,
      "question": "How do you implement a GNN in PyTorch Geometric?",
      "answer": "```python\nimport torch\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GCNConv\n\nclass GNN(torch.nn.Module):\n    def __init__(self, in_channels, hidden_channels, out_channels):\n        super().__init__()\n        self.conv1 = GCNConv(in_channels, hidden_channels)\n        self.conv2 = GCNConv(hidden_channels, out_channels)\n    \n    def forward(self, x, edge_index):\n        # First layer\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x = F.dropout(x, p=0.5, training=self.training)\n        \n        # Second layer\n        x = self.conv2(x, edge_index)\n        return x\n\nmodel = GNN(num_features, 64, num_classes)\n```"
    },
    {
      "id": 238,
      "question": "How do you handle batching in PyTorch Geometric?",
      "answer": "PyG creates **disjoint unions** of graphs for batching.\n\n**Using DataLoader:**\n```python\nfrom torch_geometric.loader import DataLoader\n\ndataset = [data1, data2, data3, ...]  # List of Data objects\nloader = DataLoader(dataset, batch_size=32, shuffle=True)\n\nfor batch in loader:\n    # batch.x: all node features concatenated\n    # batch.edge_index: all edges (with offset indices)\n    # batch.batch: assigns each node to its graph\n    out = model(batch.x, batch.edge_index)\n```\n\n**batch tensor:**\n```python\nbatch.batch = [0, 0, 0, 1, 1, 2, 2, 2, 2]\n# Nodes 0-2 from graph 0\n# Nodes 3-4 from graph 1\n# Nodes 5-8 from graph 2\n```"
    },
    {
      "id": 239,
      "question": "What is global_add_pool and when do you use it?",
      "answer": "**global_add_pool** aggregates node features to graph-level by summing.\n\n```python\nfrom torch_geometric.nn import global_add_pool\n\n# Node embeddings: [num_nodes, hidden_dim]\nnode_embeddings = gnn(x, edge_index)\n\n# Graph embedding: [batch_size, hidden_dim]\ngraph_embeddings = global_add_pool(node_embeddings, batch)\n```\n\n**When to use:**\n- Graph classification\n- Graph regression\n- Any graph-level prediction task\n\n**Alternatives:**\n- `global_mean_pool`: Average instead of sum\n- `global_max_pool`: Max instead of sum\n- `global_attention_pool`: Learnable attention weights"
    },
    {
      "id": 240,
      "question": "What are common GNN benchmark datasets?",
      "answer": "**Node classification:**\n- **Cora, CiteSeer, PubMed**: Citation networks\n- **Reddit, OGB-Products**: Social/product graphs\n\n**Graph classification:**\n- **MUTAG, PROTEINS**: Molecular graphs\n- **IMDB-BINARY**: Social networks\n- **ZINC**: Molecular property prediction\n\n**Link prediction:**\n- **OGB-Citation2**: Citation prediction\n- **Cora, CiteSeer**: Citation link prediction\n\n**Loading in PyG:**\n```python\nfrom torch_geometric.datasets import Planetoid\n\ndataset = Planetoid(root='/tmp/Cora', name='Cora')\ndata = dataset[0]  # Single graph with train/val/test masks\n```"
    },
    {
      "id": 241,
      "question": "How do you implement custom GNN layers in PyTorch Geometric?",
      "answer": "Inherit from **MessagePassing** base class:\n\n```python\nfrom torch_geometric.nn import MessagePassing\nimport torch.nn.functional as F\n\nclass CustomConv(MessagePassing):\n    def __init__(self, in_channels, out_channels):\n        super().__init__(aggr='add')  # Aggregation: add, mean, max\n        self.lin = torch.nn.Linear(in_channels, out_channels)\n    \n    def forward(self, x, edge_index):\n        # Transform features\n        x = self.lin(x)\n        # Start propagating messages\n        return self.propagate(edge_index, x=x)\n    \n    def message(self, x_j):\n        # x_j: features of source nodes\n        return x_j\n    \n    def update(self, aggr_out):\n        # aggr_out: aggregated messages\n        return F.relu(aggr_out)\n```"
    },
    {
      "id": 242,
      "question": "What is the difference between x_i and x_j in MessagePassing?",
      "answer": "In PyG's **MessagePassing**:\n\n- **x_i**: Features of **target** nodes (receivers)\n- **x_j**: Features of **source** nodes (senders)\n\n**For edge (j → i):**\n```python\ndef message(self, x_i, x_j, edge_attr):\n    # x_i: target node features\n    # x_j: source node features\n    # edge_attr: edge features\n    \n    # Concatenate source + target\n    return torch.cat([x_i, x_j], dim=-1)\n```\n\n**Automatic indexing:**\nPyG automatically maps x_j to x[edge_index[0]] and x_i to x[edge_index[1]]"
    },
    {
      "id": 243,
      "question": "What is torch.nn.ModuleList and when should you use it?",
      "answer": "**ModuleList** stores layers in a list and registers their parameters.\n\n```python\nclass GNN(torch.nn.Module):\n    def __init__(self, num_layers):\n        super().__init__()\n        # Use ModuleList for multiple layers\n        self.convs = torch.nn.ModuleList([\n            GCNConv(hidden, hidden) for _ in range(num_layers)\n        ])\n    \n    def forward(self, x, edge_index):\n        for conv in self.convs:\n            x = conv(x, edge_index)\n            x = F.relu(x)\n        return x\n```\n\n**Why not regular list?**\n- Regular list: parameters NOT registered\n- ModuleList: parameters visible to optimizer\n- ModuleDict: for dictionary of modules"
    },
    {
      "id": 244,
      "question": "How do you implement skip connections in GNNs?",
      "answer": "**Skip connections** (like ResNet) help with deep GNNs.\n\n**Method 1: Residual connection**\n```python\ndef forward(self, x, edge_index):\n    identity = x\n    x = self.conv(x, edge_index)\n    x = F.relu(x)\n    x = x + identity  # Add skip connection\n    return x\n```\n\n**Method 2: Jumping Knowledge**\n```python\nclass JKNet(torch.nn.Module):\n    def forward(self, x, edge_index):\n        xs = [x]\n        for conv in self.convs:\n            x = conv(x, edge_index)\n            xs.append(x)\n        # Concatenate all layer outputs\n        return torch.cat(xs, dim=-1)\n```"
    },
    {
      "id": 245,
      "question": "What is the torch.nn.functional vs torch.nn difference?",
      "answer": "**torch.nn (classes):**\n- Stateful layers with learnable parameters\n- Must instantiate as objects\n- Parameters automatically tracked\n\n```python\nself.dropout = nn.Dropout(p=0.5)\nself.linear = nn.Linear(128, 64)\n```\n\n**torch.nn.functional (functions):**\n- Stateless operations\n- No learnable parameters\n- Called directly\n\n```python\nx = F.dropout(x, p=0.5, training=self.training)\nx = F.relu(x)\n```\n\n**Rule of thumb:**\n- Learnable parameters → use nn (Linear, Conv, BatchNorm)\n- No parameters → use F (relu, dropout, softmax)"
    },
    {
      "id": 246,
      "question": "What are node embeddings and how are they different from node features?",
      "answer": "**Node features (x):**\n- Input properties of nodes\n- Given/predetermined\n- Example: atom type, user age, word index\n\n**Node embeddings (h):**\n- Learned representations\n- Output of GNN layers\n- Capture graph structure + features\n- Used for downstream tasks\n\n**For nodes without features:**\n```python\n# Learn embeddings from scratch\nself.embedding = nn.Embedding(num_nodes, embedding_dim)\nx = self.embedding(node_ids)\n```\n\n**Example:** In word2vec, words have no initial features; embeddings are learned entirely from context."
    },
    {
      "id": 247,
      "question": "How do you handle imbalanced node classification in GNNs?",
      "answer": "**Techniques:**\n\n**1. Weighted loss:**\n```python\nclass_weights = compute_class_weight(\n    'balanced', \n    classes=np.unique(y_train), \n    y=y_train\n)\ncriterion = nn.CrossEntropyLoss(\n    weight=torch.tensor(class_weights, dtype=torch.float)\n)\n```\n\n**2. Oversampling minority nodes:**\n- Use RandomOverSampler on node indices\n\n**3. GraphSMOTE:**\n- SMOTE adapted for graphs\n- Generates synthetic minority nodes\n\n**4. Focal Loss:**\n- Focus on hard-to-classify examples"
    },
    {
      "id": 248,
      "question": "What is the neighbor sampling strategy and why is it important?",
      "answer": "**Neighbor sampling** limits the number of neighbors to aggregate from.\n\n**Problem without sampling:**\n- Full neighborhood: exponential growth\n- 2-layer GNN: node aggregates from ~degree² neighbors\n- Huge graphs become intractable\n\n**Solution:**\n```python\nfrom torch_geometric.loader import NeighborLoader\n\nloader = NeighborLoader(\n    data,\n    num_neighbors=[10, 5],  # Sample 10 at layer 1, 5 at layer 2\n    batch_size=128,\n    input_nodes=train_mask\n)\n```\n\n**Benefits:**\n- Constant computational cost\n- Scalable to billions of nodes\n- Used by GraphSAGE, PinSAGE"
    },
    {
      "id": 249,
      "question": "What are common applications of GNNs?",
      "answer": "**Drug Discovery:**\n- Molecular property prediction\n- Drug-drug interaction\n- Protein structure prediction\n\n**Recommendation Systems:**\n- User-item bipartite graphs\n- Social recommendations (Pinterest PinSAGE)\n\n**Computer Vision:**\n- Scene graph generation\n- Point cloud classification\n- 3D mesh processing\n\n**NLP:**\n- Knowledge graph completion\n- Semantic parsing\n- Document classification with citation graphs\n\n**Traffic:**\n- Traffic prediction (roads as graph)\n- ETA estimation\n\n**Social Networks:**\n- Community detection\n- Influence prediction\n- Fraud detection"
    },
    {
      "id": 250,
      "question": "What is the difference between node2vec and GNNs?",
      "answer": "**node2vec:**\n- **Unsupervised** graph embedding method\n- Uses random walks + Skip-gram\n- Produces fixed embeddings (transductive)\n- No learnable parameters after training\n- Cannot generalize to new nodes\n\n**GNNs:**\n- **Supervised** (or semi-supervised) learning\n- Uses message passing\n- Generates embeddings via neural network\n- Inductive: can embed new nodes\n- Parameters learned end-to-end for task\n\n**When to use node2vec:**\n- No labels available\n- Need pre-trained embeddings\n- Simple baseline\n\n**When to use GNNs:**\n- Have labels for downstream task\n- Need to generalize to new graphs\n- Want end-to-end learning"
    },
    {
      "id": 251,
      "question": "What is the difference between WHERE and HAVING in SQL?",
      "answer": "**WHERE:**\n- Filters rows BEFORE grouping\n- Cannot use aggregate functions\n- Acts on individual rows\n\n```sql\nSELECT * FROM employees WHERE salary > 50000;\n```\n\n**HAVING:**\n- Filters groups AFTER GROUP BY\n- Can use aggregate functions\n- Acts on grouped results\n\n```sql\nSELECT dept, AVG(salary)\nFROM employees\nGROUP BY dept\nHAVING AVG(salary) > 60000;\n```"
    },
    {
      "id": 252,
      "question": "What is a SQL subquery and when would you use it?",
      "answer": "A **subquery** is a query nested inside another query.\n\n**Types:**\n\n**Scalar subquery (returns single value):**\n```sql\nSELECT name FROM employees\nWHERE salary > (SELECT AVG(salary) FROM employees);\n```\n\n**Row subquery:**\n```sql\nSELECT * FROM employees\nWHERE (dept, salary) IN (SELECT dept, MAX(salary) FROM employees GROUP BY dept);\n```\n\n**Use cases:**\n- When you need intermediate calculations\n- To filter based on aggregated data\n- As alternative to JOINs (sometimes less efficient)"
    },
    {
      "id": 253,
      "question": "What is the difference between UNION and UNION ALL?",
      "answer": "**UNION:**\n- Combines results and removes duplicates\n- Slower (requires sorting)\n- Returns distinct rows\n\n**UNION ALL:**\n- Combines all results including duplicates\n- Faster (no deduplication)\n- Returns all rows\n\n```sql\n-- UNION: returns [1, 2, 3]\nSELECT 1 UNION SELECT 2 UNION SELECT 1;\n\n-- UNION ALL: returns [1, 2, 1]\nSELECT 1 UNION ALL SELECT 2 UNION ALL SELECT 1;\n```\n\n**Rule:** Use UNION ALL when you know there are no duplicates or duplicates don't matter."
    },
    {
      "id": 254,
      "question": "What is a Common Table Expression (CTE) in SQL?",
      "answer": "A **CTE** (WITH clause) creates a temporary named result set.\n\n```sql\nWITH high_earners AS (\n    SELECT dept, AVG(salary) as avg_sal\n    FROM employees\n    GROUP BY dept\n    HAVING AVG(salary) > 70000\n)\nSELECT e.name, e.dept, he.avg_sal\nFROM employees e\nJOIN high_earners he ON e.dept = he.dept;\n```\n\n**Advantages:**\n- Improves readability\n- Can be referenced multiple times\n- Enables recursive queries\n- Better than subqueries for complex logic\n\n**Recursive CTE:** Can reference itself (useful for hierarchies, graphs)"
    },
    {
      "id": 255,
      "question": "What is the difference between DELETE, TRUNCATE, and DROP?",
      "answer": "**DELETE:**\n- Removes rows based on condition\n- Can be rolled back (transaction)\n- Triggers fire\n- Slower (row-by-row)\n```sql\nDELETE FROM table WHERE condition;\n```\n\n**TRUNCATE:**\n- Removes ALL rows\n- Cannot be rolled back (in most DBs)\n- No triggers\n- Fast (deallocates pages)\n```sql\nTRUNCATE TABLE table_name;\n```\n\n**DROP:**\n- Removes entire table structure + data\n- Cannot be rolled back\n```sql\nDROP TABLE table_name;\n```"
    },
    {
      "id": 256,
      "question": "What is data imputation and what are common strategies?",
      "answer": "**Data imputation**: Filling missing values in a dataset.\n\n**Common strategies:**\n\n**1. Statistical:**\n- Mean/median/mode imputation\n- Forward fill / backward fill (time series)\n\n**2. Model-based:**\n- KNN imputation: use K nearest neighbors' average\n- Regression imputation: predict missing values\n- MICE (Multiple Imputation by Chained Equations)\n\n**3. Domain-specific:**\n- Use business logic\n- Create \"missing\" category for categorical\n\n**4. Deletion:**\n- Remove rows (if few)\n- Remove columns (if too many missing)\n\n**Best practice:** Analyze missingness pattern first (MCAR, MAR, MNAR)"
    },
    {
      "id": 257,
      "question": "What is feature engineering and why is it important?",
      "answer": "**Feature engineering**: Creating new features or transforming existing ones to improve model performance.\n\n**Techniques:**\n\n**1. Transformation:**\n- Log transform (for skewed data)\n- Polynomial features (x²,  x³)\n- Binning/discretization\n\n**2. Extraction:**\n- Date → day_of_week, month, is_weekend\n- Text → length, word count, sentiment\n\n**3. Encoding:**\n- One-hot encoding (categorical)\n- Target encoding\n- Ordinal encoding\n\n**4. Interaction:**\n- Feature crosses (A × B)\n- Ratios (A / B)\n\n**Why important:**\n- Often more impactful than algorithm choice\n- Captures domain knowledge\n- Can make non-linear relationships linear"
    },
    {
      "id": 258,
      "question": "What is the curse of dimensionality?",
      "answer": "As the number of features increases, the **volume of the feature space** grows exponentially, making data sparse.\n\n**Problems:**\n\n1. **Distance becomes meaningless**: All points are equidistant in high dimensions\n2. **Overfitting**: More features than samples\n3. **Computational cost**: Exponential growth\n4. **Data requirements**: Need exponentially more data\n\n**Example:**\n- 10 points cover [0,1]: spacing = 0.1\n- 10 points cover [0,1]²: spacing = 0.33\n- 10 points cover [0,1]¹⁰: spacing = 0.79\n\n**Solutions:**\n- Feature selection\n- Dimensionality reduction (PCA, t-SNE)\n- Regularization"
    },
    {
      "id": 259,
      "question": "What is the difference between normalization and standardization?",
      "answer": "**Normalization (Min-Max Scaling):**\n- Scales to [0, 1] range\n- Formula: (x - min) / (max - min)\n- Sensitive to outliers\n\n```python\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nX_norm = scaler.fit_transform(X)\n```\n\n**Standardization (Z-score):**\n- Scales to mean=0, std=1\n- Formula: (x - μ) / σ\n- Less sensitive to outliers\n\n```python\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_std = scaler.fit_transform(X)\n```\n\n**When to use:**\n- Normalization: bounded features (neural networks)\n- Standardization: algorithms assuming normal distribution (SVM, KNN)"
    },
    {
      "id": 260,
      "question": "What is one-hot encoding and when should you use it?",
      "answer": "**One-hot encoding** converts categorical variables into binary vectors.\n\n**Example:**\nColor: [Red, Green, Blue] →\n```\nRed   → [1, 0, 0]\nGreen → [0, 1, 0]\nBlue  → [0, 0, 1]\n```\n\n**Implementation:**\n```python\nimport pandas as pd\npd.get_dummies(df['color'], drop_first=False)\n\nfrom sklearn.preprocessing import OneHotEncoder\nencoder = OneHotEncoder(sparse=False)\nencoded = encoder.fit_transform(df[['color']])\n```\n\n**When to use:**\n- Nominal categorical variables (no order)\n- Tree-based models handle it well\n\n**Avoid when:**\n- High cardinality (too many unique values)\n- Ordinal variables (use ordinal encoding instead)\n\n**Note:** drop_first=True to avoid multicollinearity"
    },
    {
      "id": 261,
      "question": "What is an SQL index and why is it important?",
      "answer": "An **index** is a data structure that improves query speed by providing quick lookups.\n\n**How it works:**\n- Like a book index: maps values to row locations\n- Usually implemented as B-tree or hash table\n\n**Benefits:**\n- Fast SELECT queries with WHERE, JOIN, ORDER BY\n- Unique indexes enforce uniqueness\n\n**Costs:**\n- Slower INSERT/UPDATE/DELETE (index must be updated)\n- Takes storage space\n\n**When to index:**\n- Primary keys (auto-indexed)\n- Foreign keys\n- Columns in WHERE/JOIN clauses\n- Columns in ORDER BY\n\n**Avoid indexing:**\n- Small tables\n- Columns with many updates\n- Columns with low cardinality (e.g., boolean)"
    },
    {
      "id": 262,
      "question": "What is a PRIMARY KEY vs FOREIGN KEY?",
      "answer": "**PRIMARY KEY:**\n- Uniquely identifies each row in a table\n- Cannot be NULL\n- Only ONE per table\n\n```sql\nCREATE TABLE users (\n    user_id INT PRIMARY KEY,\n    name VARCHAR(100)\n);\n```\n\n**FOREIGN KEY:**\n- References a primary key in another table\n- Creates relationship between tables\n- Can be NULL (optional relationship)\n- Can have multiple per table\n\n```sql\nCREATE TABLE orders (\n    order_id INT PRIMARY KEY,\n    user_id INT,\n    FOREIGN KEY (user_id) REFERENCES users(user_id)\n);\n```\n\n**Referential integrity:** FK ensures referenced row exists"
    },
    {
      "id": 263,
      "question": "What is tokenization in NLP?",
      "answer": "**Tokenization**: Breaking text into smaller units (tokens).\n\n**Types:**\n\n**1. Word tokenization:**\n```python\ntext = \"Hello, world!\"\ntokens = [\"Hello\", \",\", \"world\", \"!\"]\n```\n\n**2. Sentence tokenization:**\n```python\ntext = \"Hello world. How are you?\"\ntokens = [\"Hello world.\", \"How are you?\"]\n```\n\n**3. Subword tokenization (BPE, WordPiece):**\n```python\n\"unhappiness\" → [\"un\", \"happiness\"]\n```\n\n**Libraries:**\n```python\nimport nltk\nnltk.word_tokenize(\"Hello world!\")\n\nfrom transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n```\n\n**Why important:** First step in NLP pipeline"
    },
    {
      "id": 264,
      "question": "What are stop words and should you always remove them?",
      "answer": "**Stop words**: Common words with little semantic value (\"the\", \"is\", \"at\", \"a\").\n\n**Removal benefits:**\n- Reduces feature space\n- Faster training\n- Less noise\n\n```python\nfrom nltk.corpus import stopwords\nstop_words = set(stopwords.words('english'))\nfiltered = [w for w in tokens if w not in stop_words]\n```\n\n**When to remove:**\n- Topic modeling\n- Document classification\n- Information retrieval\n\n**When NOT to remove:**\n- Sentiment analysis (\"not good\" vs \"good\")\n- Question answering\n- Machine translation\n- Modern transformers (BERT handles them)\n\n**Modern approach:** Let the model decide (use transformers)"
    },
    {
      "id": 265,
      "question": "What is attention in the context of transformers?",
      "answer": "**Attention** computes relevance between all tokens in a sequence.\n\n**Self-attention mechanism:**\n1. Convert each token to Query (Q), Key (K), Value (V)\n2. Compute similarity: Q·Kᵀ\n3. Apply softmax to get weights\n4. Weighted sum of Values\n\n**Formula:**\n```\nAttention(Q,K,V) = softmax(QKᵀ/√dₖ) V\n```\n\n**Example:**\n\"The cat sat on the mat\"\n- \"cat\" attends strongly to \"sat\" (subject-verb)\n- \"sat\" attends to \"mat\" (verb-object)\n\n**Multi-head attention:**\n- Multiple attention operations in parallel\n- Each head learns different patterns\n\n**Why powerful:** Can capture long-range dependencies"
    },
    {
      "id": 266,
      "question": "What is the difference between encoder and decoder in transformers?",
      "answer": "**Encoder:**\n- Processes input sequence\n- Bidirectional (sees full context)\n- Uses self-attention\n- Output: contextualized representations\n- Example: BERT\n\n**Decoder:**\n- Generates output sequence\n- Unidirectional (autoregressive)\n- Uses masked self-attention + cross-attention\n- Output: predicted tokens\n- Example: GPT\n\n**Encoder-Decoder (Seq2Seq):**\n- Encoder processes source (e.g., English)\n- Decoder generates target (e.g., French)\n- Decoder attends to encoder outputs\n- Example: T5, BART\n\n**Use cases:**\n- Encoder-only: Classification, NER\n- Decoder-only: Text generation\n- Encoder-Decoder: Translation, summarization"
    },
    {
      "id": 267,
      "question": "What is transfer learning in NLP?",
      "answer": "**Transfer learning**: Pre-train on large corpus, fine-tune on specific task.\n\n**Two-step process:**\n\n**1. Pre-training (unsupervised):**\n- Train on huge text corpus\n- Task: Masked Language Model (BERT) or Next Token Prediction (GPT)\n- Learns general language understanding\n\n**2. Fine-tuning (supervised):**\n- Train on task-specific labeled data\n- Adjust all weights\n- Much faster than training from scratch\n\n**Benefits:**\n- Requires less labeled data\n- Better performance\n- Faster convergence\n\n```python\nfrom transformers import AutoModelForSequenceClassification\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"bert-base-uncased\", \n    num_labels=2\n)\n# Fine-tune on your data\n```"
    },
    {
      "id": 268,
      "question": "What is the SQL CASE statement and when do you use it?",
      "answer": "**CASE** is SQL's if-then-else conditional.\n\n**Simple CASE:**\n```sql\nSELECT name,\n    CASE grade\n        WHEN 'A' THEN 'Excellent'\n        WHEN 'B' THEN 'Good'\n        ELSE 'Needs improvement'\n    END as feedback\nFROM students;\n```\n\n**Searched CASE:**\n```sql\nSELECT name, salary,\n    CASE\n        WHEN salary >= 100000 THEN 'High'\n        WHEN salary >= 50000 THEN 'Medium'\n        ELSE 'Low'\n    END as salary_bracket\nFROM employees;\n```\n\n**Use cases:**\n- Creating derived columns\n- Conditional aggregation\n- Pivot tables\n\n**Aggregate with CASE:**\n```sql\nSELECT\n    COUNT(CASE WHEN status='active' THEN 1 END) as active_count\nFROM users;\n```"
    },
    {
      "id": 269,
      "question": "What is positional encoding in transformers?",
      "answer": "**Problem:** Transformers have no inherent notion of token order.\n\n**Solution:** Add positional information to embeddings.\n\n**Sinusoidal encoding (original Transformer):**\n```\nPE(pos, 2i) = sin(pos / 10000^(2i/d))\nPE(pos, 2i+1) = cos(pos / 10000^(2i/d))\n```\n\nWhere:\n- pos = position in sequence\n- i = dimension\n- d = embedding dimension\n\n**Properties:**\n- Deterministic\n- Can extrapolate to longer sequences\n- Smooth transitions\n\n**Learned positional embeddings:**\n- BERT uses learned embeddings\n- Limited to max_length seen during training\n\n**Result:** Token embedding = word embedding + positional encoding"
    },
    {
      "id": 270,
      "question": "What is word embedding and how does it differ from one-hot encoding?",
      "answer": "**One-hot encoding:**\n- Sparse vector (size = vocabulary size)\n- Each word is independent\n- No semantic similarity\n```python\n\"cat\" → [0,0,0,1,0,0,...,0]  # 10,000 dimensions\n\"dog\" → [0,0,0,0,1,0,...,0]\n```\n\n**Word embedding:**\n- Dense vector (typically 100-300 dimensions)\n- Captures semantic similarity\n- Similar words have similar vectors\n```python\n\"cat\" → [0.2, -0.5, 0.8, ...]  # 300 dimensions\n\"dog\" → [0.3, -0.4, 0.7, ...]  # Similar to cat\n```\n\n**Popular embeddings:**\n- Word2Vec (CBOW, Skip-gram)\n- GloVe\n- FastText\n- Contextual: BERT, ELMo (different embedding per context)\n\n**Advantage:** Dimensionality reduction + semantic meaning"
    },
    {
      "id": 271,
      "question": "What is MLOps and why is it important?",
      "answer": "**MLOps** (Machine Learning Operations) is a set of practices to deploy and maintain ML models in production.\n\n**Key components:**\n\n1. **Continuous Integration (CI):**\n   - Automated testing of code & data\n   - Version control for models & datasets\n\n2. **Continuous Deployment (CD):**\n   - Automated model deployment\n   - A/B testing in production\n\n3. **Monitoring:**\n   - Model performance tracking\n   - Data drift detection\n   - Retraining triggers\n\n**Why important:**\n- Reproducibility\n- Scalability\n- Faster iteration\n- Reduced deployment time\n\n**Tools:** MLflow, Kubeflow, Weights & Biases, DVC"
    },
    {
      "id": 272,
      "question": "What is model versioning and why is it necessary?",
      "answer": "**Model versioning** tracks different iterations of ML models.\n\n**What to version:**\n- Model weights/parameters\n- Training code\n- Hyperparameters\n- Training data snapshot\n- Dependencies (requirements.txt)\n- Metrics & evaluation results\n\n**Why necessary:**\n- **Reproducibility**: Recreate exact model\n- **Rollback**: Revert to previous version if new model fails\n- **Comparison**: Track performance over time\n- **Compliance**: Audit trail for regulated industries\n\n**Tools:**\n```python\n# MLflow\nimport mlflow\nwith mlflow.start_run():\n    mlflow.log_param(\"lr\", 0.01)\n    mlflow.log_metric(\"accuracy\", 0.95)\n    mlflow.sklearn.log_model(model, \"model\")\n\n# DVC\n# dvc add model.pkl\n# git commit -m \"model v2.0\"\n```"
    },
    {
      "id": 273,
      "question": "What is data drift and how do you detect it?",
      "answer": "**Data drift**: When input data distribution changes over time, degrading model performance.\n\n**Types:**\n\n**1. Covariate shift:** P(X) changes, P(Y|X) stays same\n   - Example: More young users than before\n\n**2. Prior probability shift:** P(Y) changes\n   - Example: Fraud rate increases\n\n**3. Concept drift:** P(Y|X) changes\n   - Example: User preferences change\n\n**Detection methods:**\n\n**Statistical tests:**\n- Kolmogorov-Smirnov test\n- Chi-square test\n- Population Stability Index (PSI)\n\n**Monitoring:**\n```python\n# Compare distributions\nfrom scipy.stats import ks_2samp\nstatistic, p_value = ks_2samp(train_data, prod_data)\nif p_value < 0.05:\n    print(\"Drift detected!\")\n```\n\n**Action:** Retrain model with recent data"
    },
    {
      "id": 274,
      "question": "What is A/B testing for ML models?",
      "answer": "**A/B testing**: Compare two model versions in production to determine which performs better.\n\n**Setup:**\n- **Control group (A)**: Current model\n- **Treatment group (B)**: New model\n- Randomly assign users to each group\n\n**Process:**\n```python\nif user_id % 2 == 0:\n    prediction = model_A.predict(features)\nelse:\n    prediction = model_B.predict(features)\n```\n\n**Metrics to track:**\n- Business KPIs (revenue, conversion rate)\n- Model metrics (accuracy, latency)\n- User engagement\n\n**Statistical significance:**\n- Run until enough samples\n- Calculate p-value\n- Ensure practical significance (not just statistical)\n\n**Decision:** Deploy B if significantly better, else keep A\n\n**Advanced:** Multi-armed bandits for dynamic allocation"
    },
    {
      "id": 275,
      "question": "What is named entity recognition (NER)?",
      "answer": "**NER**: Identifying and classifying named entities in text.\n\n**Common entity types:**\n- PERSON: \"Barack Obama\"\n- LOCATION: \"Paris\"\n- ORGANIZATION: \"Google\"\n- DATE: \"January 1, 2024\"\n- MONEY: \"$100\"\n\n**Example:**\n```\nInput: \"Apple Inc. was founded by Steve Jobs in California.\"\n\nOutput:\n- Apple Inc. → ORGANIZATION\n- Steve Jobs → PERSON\n- California → LOCATION\n```\n\n**Approaches:**\n\n**1. Rule-based:** Regex, gazetteers\n**2. ML-based:** CRF, HMM\n**3. Deep learning:** BiLSTM-CRF, BERT\n\n**Implementation:**\n```python\nimport spacy\nnlp = spacy.load(\"en_core_web_sm\")\ndoc = nlp(\"Apple Inc. was founded by Steve Jobs\")\nfor ent in doc.ents:\n    print(ent.text, ent.label_)\n```"
    },
    {
      "id": 276,
      "question": "What is the difference between extractive and abstractive summarization?",
      "answer": "**Extractive summarization:**\n- Selects and copies sentences from source\n- No new text generation\n- Guaranteed grammatically correct\n- Simpler, faster\n\n**Example:**\nSource: \"The cat sat on the mat. It was very comfortable. The mat was soft.\"\nSummary: \"The cat sat on the mat. The mat was soft.\"\n\n**Abstractive summarization:**\n- Generates new sentences\n- Paraphrases and condenses\n- More human-like\n- Harder, may have errors\n\n**Example:**\nSource: \"The cat sat on the mat. It was very comfortable. The mat was soft.\"\nSummary: \"A cat comfortably rested on a soft mat.\"\n\n**Models:**\n- Extractive: TextRank, LexRank\n- Abstractive: T5, BART, GPT\n\n**Modern approach:** Abstractive with transformers"
    },
    {
      "id": 277,
      "question": "What is fine-tuning a pre-trained model?",
      "answer": "**Fine-tuning**: Adapting a pre-trained model to a specific task by continuing training.\n\n**Steps:**\n\n1. **Load pre-trained model:**\n```python\nfrom transformers import AutoModelForSequenceClassification\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"bert-base-uncased\",\n    num_labels=3\n)\n```\n\n2. **Prepare task-specific data:**\n- Sentiment analysis\n- Question answering\n- NER\n\n3. **Train with small learning rate:**\n```python\noptimizer = AdamW(model.parameters(), lr=2e-5)\n```\n\n**Why low learning rate?**\n- Preserve pre-trained knowledge\n- Avoid catastrophic forgetting\n\n**Types:**\n- **Full fine-tuning**: Update all weights\n- **Feature extraction**: Freeze base, train only head\n- **LoRA**: Low-rank adaptation (efficient)"
    },
    {
      "id": 278,
      "question": "What is CI/CD in the context of ML?",
      "answer": "**CI/CD for ML**: Automated pipeline for testing, validating, and deploying models.\n\n**Continuous Integration (CI):**\n- **Code tests**: Unit tests for data processing\n- **Data validation**: Schema checks, distribution tests\n- **Model tests**: Performance on hold-out set\n\n```yaml\n# .github/workflows/ci.yml\nsteps:\n  - name: Test data pipeline\n    run: pytest tests/\n  - name: Validate model\n    run: python validate_model.py\n```\n\n**Continuous Deployment (CD):**\n- Automated model deployment\n- Gradual rollout (canary, blue-green)\n- Monitoring & alerts\n\n**Tools:**\n- GitHub Actions, GitLab CI\n- Jenkins\n- Kubeflow Pipelines\n- AWS SageMaker Pipelines\n\n**Benefits:**\n- Catch bugs early\n- Faster iteration\n- Reproducible deployments"
    },
    {
      "id": 279,
      "question": "What is the transformer architecture's attention mechanism complexity?",
      "answer": "**Self-attention complexity:** O(n²·d)\n\nWhere:\n- n = sequence length\n- d = embedding dimension\n\n**Why O(n²)?**\n1. Compute QKᵀ: (n×d) × (d×n) = (n×n) matrix\n2. Each token attends to all other tokens\n3. Quadratic in sequence length\n\n**Memory bottleneck:**\n- Limits max sequence length (~512-1024 tokens)\n- GPUs run out of memory for long sequences\n\n**Solutions for long sequences:**\n\n**1. Sparse attention:**\n- Longformer: Local + global attention\n- BigBird: Random + window + global\n\n**2. Linear attention:**\n- Linformer: O(n) complexity\n- Performer: Kernel-based approximation\n\n**3. Recurrence:**\n- Transformer-XL: Segment-level recurrence\n\n**Trade-off:** Efficiency vs. ability to capture long-range dependencies"
    },
    {
      "id": 280,
      "question": "What is prompt engineering for LLMs?",
      "answer": "**Prompt engineering**: Crafting inputs to guide LLM behavior without fine-tuning.\n\n**Techniques:**\n\n**1. Zero-shot:**\n```\n\"Classify the sentiment: I love this product!\"\n```\n\n**2. Few-shot (in-context learning):**\n```\n\"Classify sentiment:\nI love it → positive\nI hate it → negative\nIt's okay → neutral\nI love this product! →\n```\n\n**3. Chain-of-thought:**\n```\n\"Let's think step by step:\n1. First, identify key words...\n2. Then, analyze the tone...\"\n```\n\n**4. Role prompting:**\n```\n\"You are an expert data scientist. Explain regularization.\"\n```\n\n**5. Instruction following:**\n```\n\"Answer in 3 bullet points. Be concise.\"\n```\n\n**Best practices:**\n- Be specific\n- Provide examples\n- Specify format\n- Iterate on prompts"
    }
  ],
  "themes": [
    {
      "id": "fondamentaux",
      "name": "Concepts Fondamentaux",
      "question_ids": [
        1,
        2,
        3,
        4,
        91,
        92,
        93,
        94,
        95,
        17,
        18,
        258
      ]
    },
    {
      "id": "statistiques",
      "name": "Statistiques",
      "question_ids": [
        5,
        6,
        7,
        8,
        9,
        10,
        11,
        12,
        13,
        14,
        15,
        16,
        17,
        18,
        19,
        20,
        21,
        22,
        23,
        24,
        25,
        26,
        27,
        96,
        97,
        98,
        99
      ]
    },
    {
      "id": "analyse",
      "name": "Analyse de Données",
      "question_ids": [
        28,
        29,
        30,
        31,
        32,
        33,
        34,
        35,
        36,
        37,
        38,
        39,
        100,
        101,
        102,
        103,
        104,
        256,
        257,
        258,
        259,
        260,
        261
      ]
    },
    {
      "id": "sql",
      "name": "SQL",
      "question_ids": [
        105,
        106,
        107,
        108,
        109,
        110,
        111,
        251,
        252,
        253,
        254,
        255,
        261,
        262,
        268
      ]
    },
    {
      "id": "ml",
      "name": "Machine Learning",
      "question_ids": [
        40,
        41,
        42,
        43,
        44,
        45,
        46,
        47,
        48,
        49,
        50,
        112,
        113,
        114,
        115,
        116,
        117,
        118,
        119,
        120,
        121,
        122,
        123,
        124,
        125,
        126,
        127,
        17,
        18,
        19,
        4,
        256,
        257,
        258,
        259,
        260,
        273,
        274
      ]
    },
    {
      "id": "regression",
      "name": "Régression Linéaire et Logistique",
      "question_ids": [
        128,
        129,
        130,
        131,
        132,
        133,
        134,
        135,
        136,
        137,
        138,
        139,
        140,
        141,
        19
      ]
    },
    {
      "id": "nlp",
      "name": "Natural Language Processing",
      "question_ids": [
        142,
        143,
        144,
        145,
        263,
        264,
        267,
        270,
        275,
        276,
        277,
        280
      ]
    },
    {
      "id": "dl",
      "name": "Deep Learning",
      "question_ids": [
        69,
        70,
        71,
        72,
        73,
        74,
        75,
        76,
        77,
        78,
        79,
        80,
        81,
        82,
        83,
        84,
        85,
        86,
        87,
        89,
        90,
        146,
        147,
        148,
        149,
        150,
        151,
        152,
        153,
        154,
        155,
        156,
        157,
        158,
        159,
        160,
        124,
        17,
        18,
        19,
        259,
        265,
        266,
        267,
        269,
        270,
        277,
        279
      ]
    },
    {
      "id": "transformers",
      "name": "Transformers",
      "question_ids": [
        161,
        162,
        163,
        164,
        165,
        166,
        265,
        266,
        267,
        269,
        276,
        277,
        279,
        280
      ]
    },
    {
      "id": "pytorch",
      "name": "PyTorch",
      "question_ids": [
        201,
        202,
        203,
        204,
        205,
        206,
        207,
        208,
        209,
        210,
        211,
        212,
        213,
        214,
        215,
        216,
        217,
        218,
        219,
        220,
        124,
        85,
        89,
        90,
        76,
        148,
        147,
        150,
        79,
        80,
        81,
        154,
        155,
        78
      ]
    },
    {
      "id": "gnn",
      "name": "Graph Neural Networks",
      "question_ids": [
        221,
        222,
        223,
        224,
        225,
        226,
        227,
        228,
        229,
        230,
        231,
        232,
        233,
        234,
        235,
        236,
        237,
        238,
        239,
        240,
        241,
        242,
        243,
        244,
        245,
        246,
        247,
        248,
        249,
        250
      ]
    },
    {
      "id": "mlops",
      "name": "MLOps",
      "question_ids": [
        167,
        168,
        169,
        170,
        271,
        272,
        273,
        274,
        278
      ]
    }
  ]
}